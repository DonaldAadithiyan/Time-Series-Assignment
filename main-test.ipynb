{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16980b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sales - Min: 0.00, Max: 124717.00\n",
      "Negative sales: 0, Zero sales: 939130\n",
      ">>> Train period: 2013-01-01 00:00:00 to 2017-07-31 00:00:00\n",
      ">>> Test period: 2017-08-01 00:00:00 to 2017-08-15 00:00:00\n",
      ">>> Train samples: 2974158\n",
      ">>> Test samples: 26730\n",
      ">>> Log sales range: 0.010 to 11.734\n",
      ">>> Creating time features...\n",
      ">>> Adding other features...\n",
      ">>> Creating lag features with log transform...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# 0️⃣ Load & prepare data with train/test split\n",
    "# ============================\n",
    "merged_train_df = pd.read_csv('merged_train_df.csv')\n",
    "\n",
    "merged_train_df['date'] = pd.to_datetime(merged_train_df['date'])\n",
    "merged_train_df = merged_train_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "merged_train_df['store_nbr'] = merged_train_df['store_nbr'].astype(int)\n",
    "\n",
    "# Handle sales data quality (SAME AS MAIN.IPYNB)\n",
    "print(f\"Train sales - Min: {merged_train_df['sales'].min():.2f}, Max: {merged_train_df['sales'].max():.2f}\")\n",
    "print(f\"Negative sales: {(merged_train_df['sales'] < 0).sum()}, Zero sales: {(merged_train_df['sales'] == 0).sum()}\")\n",
    "\n",
    "# Clip negative values and add small epsilon for log (SAME AS MAIN.IPYNB)\n",
    "merged_train_df['sales'] = np.maximum(merged_train_df['sales'], 0.01)\n",
    "\n",
    "# Split data: use last 16 days as test set (like original test period)\n",
    "max_date = merged_train_df['date'].max()\n",
    "test_start_date = max_date - timedelta(days=15)  # Last 16 days for test\n",
    "\n",
    "train_merged_train_df = merged_train_df[merged_train_df['date'] <= test_start_date].copy()\n",
    "test_merged_train_df = merged_train_df[merged_train_df['date'] > test_start_date].copy()\n",
    "\n",
    "# Add LOG TRANSFORM (KEY FROM MAIN.IPYNB)\n",
    "train_merged_train_df['log_sales'] = np.log1p(train_merged_train_df['sales'])\n",
    "\n",
    "# Create test IDs for compatibility\n",
    "test_merged_train_df = test_merged_train_df.reset_index(drop=True)\n",
    "test_merged_train_df['id'] = range(len(test_merged_train_df))\n",
    "\n",
    "print(f\">>> Train period: {train_merged_train_df['date'].min()} to {train_merged_train_df['date'].max()}\")\n",
    "print(f\">>> Test period: {test_merged_train_df['date'].min()} to {test_merged_train_df['date'].max()}\")\n",
    "print(f\">>> Train samples: {len(train_merged_train_df)}\")\n",
    "print(f\">>> Test samples: {len(test_merged_train_df)}\")\n",
    "print(f\">>> Log sales range: {train_merged_train_df['log_sales'].min():.3f} to {train_merged_train_df['log_sales'].max():.3f}\")\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Advanced data preprocessing (EXACT SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['week'] = df['date'].dt.isocalendar().week\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Advanced time features\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_monday'] = (df['dayofweek'] == 0).astype(int)\n",
    "    df['is_friday'] = (df['dayofweek'] == 4).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_other_features(df):\n",
    "    \"\"\"Add non-time features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Store-family identifier\n",
    "    df['store_family'] = df['store_nbr'].astype(str) + \"_\" + df['family'].astype(str)\n",
    "    \n",
    "    # Promotion features\n",
    "    if 'onpromotion' in df.columns:\n",
    "        df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['onpromotion'] = 0\n",
    "    \n",
    "    # Holiday features\n",
    "    if 'isHoliday' in df.columns:\n",
    "        df['isHoliday'] = df['isHoliday'].fillna(0).astype(int)\n",
    "        df['promo_holiday'] = df['onpromotion'] * df['isHoliday']\n",
    "    else:\n",
    "        df['isHoliday'] = 0\n",
    "        df['promo_holiday'] = 0\n",
    "    \n",
    "    # Oil price features\n",
    "    if 'oil_price' in df.columns:\n",
    "        df['oil_price'] = df['oil_price'].fillna(method='ffill').fillna(method='bfill').fillna(100)\n",
    "    else:\n",
    "        df['oil_price'] = 100\n",
    "    \n",
    "    # Special event features\n",
    "    for col in ['earthquake_impact', 'salary_day_impact']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features_safe(df_list):\n",
    "    \"\"\"Create lag features safely for list of dataframes (one per store-family) - WITH LOG TRANSFORM\"\"\"\n",
    "    \n",
    "    result_dfs = []\n",
    "    \n",
    "    for df_group in df_list:\n",
    "        if len(df_group) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_group = df_group.copy().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Use LOG SALES for lag features (KEY FROM MAIN.IPYNB)\n",
    "        if 'log_sales' in df_group.columns:\n",
    "            target_col = 'log_sales'\n",
    "        else:\n",
    "            target_col = 'sales'  # fallback for test data\n",
    "        \n",
    "        # Simple lag features\n",
    "        df_group['sales_lag_1'] = df_group[target_col].shift(1).fillna(df_group[target_col].mean())\n",
    "        df_group['sales_lag_7'] = df_group[target_col].shift(7).fillna(df_group[target_col].mean())\n",
    "        df_group['sales_lag_14'] = df_group[target_col].shift(14).fillna(df_group[target_col].mean())\n",
    "        \n",
    "        # Simple rolling features - avoid groupby issues\n",
    "        df_group['sales_roll_7_mean'] = df_group[target_col].rolling(7, min_periods=1).mean()\n",
    "        df_group['sales_roll_14_mean'] = df_group[target_col].rolling(14, min_periods=1).mean()\n",
    "        df_group['sales_roll_7_std'] = df_group[target_col].rolling(7, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Simple trend feature\n",
    "        if len(df_group) >= 7:\n",
    "            df_group['sales_trend_7'] = (\n",
    "                df_group[target_col].rolling(7, min_periods=1).mean() / \n",
    "                df_group[target_col].rolling(14, min_periods=1).mean().shift(7).fillna(df_group[target_col].mean())\n",
    "            ).fillna(1.0)\n",
    "        else:\n",
    "            df_group['sales_trend_7'] = 1.0\n",
    "        \n",
    "        result_dfs.append(df_group)\n",
    "    \n",
    "    return result_dfs\n",
    "\n",
    "# Apply feature engineering (SAME AS MAIN.IPYNB)\n",
    "print(\">>> Creating time features...\")\n",
    "train_fe = create_time_features(train_merged_train_df)\n",
    "test_fe = create_time_features(test_merged_train_df)\n",
    "\n",
    "print(\">>> Adding other features...\")\n",
    "train_fe = add_other_features(train_fe)\n",
    "test_fe = add_other_features(test_fe)\n",
    "\n",
    "# Create lag features by processing each store-family group separately\n",
    "print(\">>> Creating lag features with log transform...\")\n",
    "\n",
    "# Split training data by store-family\n",
    "train_groups = []\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    train_groups.append(group)\n",
    "\n",
    "# Process lag features\n",
    "train_groups_with_lags = create_lag_features_safe(train_groups)\n",
    "\n",
    "# Recombine training data\n",
    "if train_groups_with_lags:\n",
    "    train_fe = pd.concat(train_groups_with_lags, ignore_index=True)\n",
    "    train_fe = train_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# For test data, create lag features using training data statistics (in log space)\n",
    "print(\">>> Creating lag features for test...\")\n",
    "\n",
    "test_with_lags = []\n",
    "for (store, family), test_group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    test_group = test_group.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Find corresponding training group\n",
    "    train_group = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_group) > 0:\n",
    "        # Use last values from training for lag initialization (in log space)\n",
    "        recent_log_sales = train_group['log_sales'].tail(14)\n",
    "        \n",
    "        # Initialize lag features\n",
    "        if len(recent_log_sales) >= 1:\n",
    "            test_group['sales_lag_1'] = recent_log_sales.iloc[-1]\n",
    "        else:\n",
    "            test_group['sales_lag_1'] = train_group['log_sales'].mean()\n",
    "            \n",
    "        if len(recent_log_sales) >= 7:\n",
    "            test_group['sales_lag_7'] = recent_log_sales.iloc[-7]\n",
    "        else:\n",
    "            test_group['sales_lag_7'] = train_group['log_sales'].mean()\n",
    "            \n",
    "        if len(recent_log_sales) >= 14:\n",
    "            test_group['sales_lag_14'] = recent_log_sales.iloc[-14]\n",
    "        else:\n",
    "            test_group['sales_lag_14'] = train_group['log_sales'].mean()\n",
    "        \n",
    "        # Rolling features - use recent statistics\n",
    "        test_group['sales_roll_7_mean'] = recent_log_sales.tail(7).mean() if len(recent_log_sales) >= 7 else train_group['log_sales'].mean()\n",
    "        test_group['sales_roll_14_mean'] = recent_log_sales.mean()\n",
    "        test_group['sales_roll_7_std'] = recent_log_sales.tail(7).std() if len(recent_log_sales) >= 7 else train_group['log_sales'].std()\n",
    "        \n",
    "        if pd.isna(test_group['sales_roll_7_std'].iloc[0]):\n",
    "            test_group['sales_roll_7_std'] = 0\n",
    "            \n",
    "        # Trend feature\n",
    "        test_group['sales_trend_7'] = 1.0\n",
    "        \n",
    "    else:\n",
    "        # No training data - use defaults (use log space defaults)\n",
    "        default_log_sales = np.log1p(1.0)\n",
    "        for col in ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_roll_7_mean', 'sales_roll_14_mean']:\n",
    "            test_group[col] = default_log_sales\n",
    "        test_group['sales_roll_7_std'] = 0.0\n",
    "        test_group['sales_trend_7'] = 1.0\n",
    "    \n",
    "    test_with_lags.append(test_group)\n",
    "\n",
    "if test_with_lags:\n",
    "    test_fe = pd.concat(test_with_lags, ignore_index=True)\n",
    "    test_fe = test_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Add aggregate features (SAME AS MAIN.IPYNB - but with log sales)\n",
    "# ============================\n",
    "print(\">>> Creating aggregate features (log space)...\")\n",
    "\n",
    "# Store level statistics from training data (using log_sales)\n",
    "store_stats = train_fe.groupby('store_nbr')['log_sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "store_stats.columns = ['store_nbr', 'store_sales_mean', 'store_sales_std', 'store_sales_median']\n",
    "store_stats['store_sales_std'] = store_stats['store_sales_std'].fillna(0)\n",
    "\n",
    "# Family level statistics\n",
    "family_stats = train_fe.groupby('family')['log_sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "family_stats.columns = ['family', 'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "family_stats['family_sales_std'] = family_stats['family_sales_std'].fillna(0)\n",
    "\n",
    "# Add to training data\n",
    "train_fe = train_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "train_fe = train_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Add to test data\n",
    "test_fe = test_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "test_fe = test_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Fill missing aggregate features\n",
    "agg_cols = ['store_sales_mean', 'store_sales_std', 'store_sales_median', \n",
    "            'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "\n",
    "for col in agg_cols:\n",
    "    overall_median = train_fe[col].median()\n",
    "    train_fe[col] = train_fe[col].fillna(overall_median)\n",
    "    test_fe[col] = test_fe[col].fillna(overall_median)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Encode categoricals (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "cat_cols = ['family']\n",
    "if all(col in train_fe.columns and col in test_fe.columns for col in ['city', 'state', 'type']):\n",
    "    cat_cols.extend(['city', 'state', 'type'])\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined_values = pd.concat([train_fe[col].astype(str), test_fe[col].astype(str)]).unique()\n",
    "    le.fit(combined_values)\n",
    "    train_fe[f'{col}_encoded'] = le.transform(train_fe[col].astype(str))\n",
    "    test_fe[f'{col}_encoded'] = le.transform(test_fe[col].astype(str))\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Feature selection (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "base_features = [\n",
    "    'year', 'month', 'day', 'dayofweek', 'quarter', 'week', 'dayofyear',\n",
    "    'is_weekend', 'is_monday', 'is_friday', 'is_month_start', 'is_month_end',\n",
    "    'is_quarter_start', 'is_quarter_end',\n",
    "    'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'day_sin', 'day_cos',\n",
    "    'onpromotion', 'isHoliday', 'promo_holiday', 'oil_price', 'earthquake_impact', 'salary_day_impact'\n",
    "]\n",
    "\n",
    "lag_features = ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', \n",
    "                'sales_roll_7_mean', 'sales_roll_14_mean', 'sales_roll_7_std', 'sales_trend_7']\n",
    "\n",
    "aggregate_features = ['store_sales_mean', 'store_sales_std', 'store_sales_median',\n",
    "                     'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "\n",
    "encoded_features = [f'{col}_encoded' for col in cat_cols]\n",
    "\n",
    "all_features = base_features + lag_features + aggregate_features + encoded_features\n",
    "available_features = [col for col in all_features if col in train_fe.columns and col in test_fe.columns]\n",
    "\n",
    "print(f\">>> Using {len(available_features)} features\")\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Random Forest training with REDUCED OVERFITTING + LOG TARGET (EXACT SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(\">>> Training Random Forest model (log space, reduced overfitting)...\")\n",
    "\n",
    "# Clean training data\n",
    "train_clean = train_fe.dropna(subset=lag_features[:3])  # Only require basic lags\n",
    "print(f\">>> Clean training samples: {len(train_clean)}\")\n",
    "\n",
    "if len(train_clean) > 500:\n",
    "    # Prepare data\n",
    "    X_train = train_clean[available_features].fillna(0)\n",
    "    y_train = train_clean['log_sales']  # TARGET IS NOW LOG_SALES!\n",
    "    X_test = test_fe[available_features].fillna(0)\n",
    "    \n",
    "    # Train Random Forest - REDUCED OVERFITTING PARAMETERS (EXACT SAME AS MAIN.IPYNB)\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=150,      # Increased from 100 for better generalization\n",
    "        max_depth=10,          # Reduced from 12 to prevent overfitting\n",
    "        min_samples_split=20,  # Increased from 10 to prevent overfitting\n",
    "        min_samples_leaf=10,   # Increased from 5 to prevent overfitting\n",
    "        max_features='sqrt',   # Added to reduce overfitting\n",
    "        bootstrap=True,        # Ensure bootstrap sampling\n",
    "        max_samples=0.8,       # Use 80% of data per tree\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (in log space)\n",
    "    rf_log_predictions = rf_model.predict(X_test)\n",
    "    \n",
    "    # Convert back to original space\n",
    "    rf_predictions = np.expm1(rf_log_predictions)  # Inverse of log1p\n",
    "    rf_predictions = np.maximum(rf_predictions, 0.01)\n",
    "    \n",
    "    print(\">>> Random Forest training completed (with log transform)\")\n",
    "    \n",
    "    # Feature importance (ADDED AS REQUESTED)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\">>> Top 15 RF features:\")\n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    rf_predictions = None\n",
    "    rf_log_predictions = None\n",
    "    print(\">>> Insufficient training data for Random Forest\")\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ ARIMA Model - ENHANCED WITH LOG TRANSFORM (EXACT SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(\">>> Adding ARIMA component (enhanced, log space)...\")\n",
    "\n",
    "def fit_enhanced_arima(train_data, forecast_periods, max_data_points=120):\n",
    "    \"\"\"Enhanced ARIMA model for time series patterns (log space)\"\"\"\n",
    "    \n",
    "    if len(train_data) < 28:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use recent data and work in log space\n",
    "        recent_data = train_data.sort_values('date').tail(max_data_points)\n",
    "        log_sales_series = recent_data['log_sales'].values\n",
    "        \n",
    "        # Simple parameter selection with more configurations\n",
    "        best_model = None\n",
    "        best_aic = float('inf')\n",
    "        \n",
    "        # Try more ARIMA configurations\n",
    "        configs = [\n",
    "            (1,1,1), (2,1,1), (1,1,2), (0,1,1), (1,1,0), (2,1,2),\n",
    "            (3,1,1), (1,1,3), (2,1,0), (0,1,2), (1,0,1), (2,0,2)  # Additional configs\n",
    "        ]\n",
    "        \n",
    "        for p, d, q in configs:\n",
    "            try:\n",
    "                model = ARIMA(log_sales_series, order=(p, d, q))\n",
    "                fitted_model = model.fit()\n",
    "                \n",
    "                if fitted_model.aic < best_aic:\n",
    "                    best_aic = fitted_model.aic\n",
    "                    best_model = fitted_model\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_model is not None:\n",
    "            log_forecast = best_model.forecast(steps=forecast_periods)\n",
    "            # Convert back to original space\n",
    "            forecast = np.expm1(log_forecast)\n",
    "            forecast = np.maximum(forecast, 0.01)\n",
    "            return forecast\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Select MORE store-family combinations for ARIMA (increased impact)\n",
    "print(\">>> Selecting combinations for ARIMA (increased coverage)...\")\n",
    "arima_combinations = []\n",
    "\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    if len(group) >= 42:  # Reduced threshold from 50 to get more combinations\n",
    "        volume = group['sales'].sum()\n",
    "        data_points = len(group)\n",
    "        score = volume * np.log(data_points)  # Simple scoring\n",
    "        arima_combinations.append((store, family, score))\n",
    "\n",
    "# Sort and take top 200 combinations (increased from 100)\n",
    "arima_combinations.sort(key=lambda x: x[2], reverse=True)\n",
    "top_arima_combinations = [x[:2] for x in arima_combinations[:200]]\n",
    "\n",
    "print(f\">>> Processing ARIMA for {len(top_arima_combinations)} combinations...\")\n",
    "\n",
    "arima_results = []\n",
    "for i, (store, family) in enumerate(top_arima_combinations):\n",
    "    if i % 40 == 0:\n",
    "        print(f\">>> ARIMA progress: {i+1}/{len(top_arima_combinations)}\")\n",
    "        \n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    test_subset = test_fe[(test_fe['store_nbr'] == store) & (test_fe['family'] == family)]\n",
    "    \n",
    "    if len(test_subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    arima_pred = fit_enhanced_arima(train_subset, len(test_subset))\n",
    "    \n",
    "    if arima_pred is not None:\n",
    "        result_df = test_subset[['id']].copy()\n",
    "        result_df['sales_arima'] = arima_pred\n",
    "        arima_results.append(result_df)\n",
    "\n",
    "# Combine ARIMA results\n",
    "if arima_results:\n",
    "    arima_df = pd.concat(arima_results, ignore_index=True)\n",
    "    print(f\">>> ARIMA predictions generated for {len(arima_df)} test samples\")\n",
    "else:\n",
    "    arima_df = pd.DataFrame(columns=['id', 'sales_arima'])\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Store-family level predictions (SAME AS MAIN.IPYNB but with log awareness)\n",
    "# ============================\n",
    "print(\">>> Creating store-family level predictions...\")\n",
    "\n",
    "store_family_preds = []\n",
    "store_family_combinations = set(\n",
    "    zip(train_fe['store_nbr'], train_fe['family'])\n",
    ").intersection(set(zip(test_fe['store_nbr'], test_fe['family'])))\n",
    "\n",
    "for i, (store_num, family) in enumerate(sorted(store_family_combinations)):\n",
    "    if i % 300 == 0:\n",
    "        print(f\">>> Processing {i+1}/{len(store_family_combinations)}\")\n",
    "    \n",
    "    train_subset = train_fe[\n",
    "        (train_fe['store_nbr'] == store_num) & (train_fe['family'] == family)\n",
    "    ].sort_values('date').tail(100)  # Use recent data\n",
    "    \n",
    "    test_subset = test_fe[\n",
    "        (test_fe['store_nbr'] == store_num) & (test_fe['family'] == family)\n",
    "    ].sort_values('date')\n",
    "    \n",
    "    if len(train_subset) < 10 or len(test_subset) == 0:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        recent_sales = train_subset['sales'].values\n",
    "        \n",
    "        # Advanced prediction combining multiple signals\n",
    "        base_pred = np.median(recent_sales[-21:]) if len(recent_sales) >= 21 else np.median(recent_sales)\n",
    "        \n",
    "        # Trend (last month vs previous month)\n",
    "        if len(recent_sales) >= 28:\n",
    "            recent_avg = np.mean(recent_sales[-14:])\n",
    "            prev_avg = np.mean(recent_sales[-28:-14])\n",
    "            trend_factor = recent_avg / (prev_avg + 0.01)\n",
    "            trend_factor = np.clip(trend_factor, 0.8, 1.3)\n",
    "        else:\n",
    "            trend_factor = 1.0\n",
    "        \n",
    "        # Seasonality (day of week)\n",
    "        seasonal_factors = []\n",
    "        train_df_subset = train_subset.copy()\n",
    "        \n",
    "        for _, row in test_subset.iterrows():\n",
    "            dow = row['dayofweek']\n",
    "            dow_sales = train_df_subset[train_df_subset['dayofweek'] == dow]['sales']\n",
    "            \n",
    "            if len(dow_sales) >= 3:\n",
    "                dow_factor = dow_sales.mean() / (train_df_subset['sales'].mean() + 0.01)\n",
    "                seasonal_factors.append(np.clip(dow_factor, 0.7, 1.5))\n",
    "            else:\n",
    "                seasonal_factors.append(1.0)\n",
    "        \n",
    "        # Promotion effect\n",
    "        promo_factors = []\n",
    "        if 'onpromotion' in test_subset.columns:\n",
    "            # Calculate promotion lift from training data\n",
    "            promo_sales = train_df_subset[train_df_subset['onpromotion'] == 1]['sales']\n",
    "            normal_sales = train_df_subset[train_df_subset['onpromotion'] == 0]['sales']\n",
    "            \n",
    "            if len(promo_sales) > 0 and len(normal_sales) > 0:\n",
    "                promo_lift = promo_sales.mean() / (normal_sales.mean() + 0.01)\n",
    "                promo_lift = np.clip(promo_lift, 1.0, 1.8)\n",
    "            else:\n",
    "                promo_lift = 1.15  # Default 15% lift\n",
    "            \n",
    "            for _, row in test_subset.iterrows():\n",
    "                if row['onpromotion'] == 1:\n",
    "                    promo_factors.append(promo_lift)\n",
    "                else:\n",
    "                    promo_factors.append(1.0)\n",
    "        else:\n",
    "            promo_factors = [1.0] * len(test_subset)\n",
    "        \n",
    "        # Combine all factors\n",
    "        predictions_array = (\n",
    "            base_pred * trend_factor * \n",
    "            np.array(seasonal_factors) * \n",
    "            np.array(promo_factors)\n",
    "        )\n",
    "        \n",
    "        predictions_array = np.maximum(predictions_array, 0.01)\n",
    "        \n",
    "    except:\n",
    "        predictions_array = np.full(len(test_subset), max(np.median(recent_sales), 0.01))\n",
    "    \n",
    "    # Store results\n",
    "    output_df = test_subset[['id']].copy()\n",
    "    output_df['sales'] = predictions_array\n",
    "    store_family_preds.append(output_df)\n",
    "\n",
    "# ============================\n",
    "# 8️⃣ Final RF+ARIMA ensemble - INCREASED ARIMA IMPACT (EXACT SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(\">>> Creating RF+ARIMA ensemble (increased ARIMA impact)...\")\n",
    "\n",
    "if store_family_preds:\n",
    "    sf_pred_df = pd.concat(store_family_preds, ignore_index=True)\n",
    "    \n",
    "    if rf_predictions is not None:\n",
    "        # Create RF prediction dataframe\n",
    "        rf_pred_df = test_fe[['id']].copy()\n",
    "        rf_pred_df['sales'] = rf_predictions\n",
    "        \n",
    "        # Merge all predictions\n",
    "        ensemble_df = sf_pred_df.merge(rf_pred_df, on='id', how='outer', suffixes=('_sf', '_rf'))\n",
    "        \n",
    "        # Add ARIMA if available\n",
    "        if len(arima_df) > 0:\n",
    "            ensemble_df = ensemble_df.merge(arima_df[['id', 'sales_arima']], on='id', how='left')\n",
    "        else:\n",
    "            ensemble_df['sales_arima'] = np.nan\n",
    "        \n",
    "        # Fill missing values\n",
    "        ensemble_df['sales_sf'] = ensemble_df['sales_sf'].fillna(ensemble_df['sales_rf'])\n",
    "        ensemble_df['sales_rf'] = ensemble_df['sales_rf'].fillna(ensemble_df['sales_sf'])\n",
    "        \n",
    "        # RF+SF ensemble - slightly more conservative\n",
    "        ensemble_df['sales'] = ensemble_df['sales_rf'] * 0.55 + ensemble_df['sales_sf'] * 0.45  # More balanced\n",
    "        \n",
    "        # Add ARIMA component where available - INCREASED IMPACT\n",
    "        arima_mask = ~pd.isna(ensemble_df['sales_arima'])\n",
    "        if arima_mask.sum() > 0:\n",
    "            # Increased ARIMA weight: 50% base ensemble + 50% ARIMA (was 70%/30%)\n",
    "            ensemble_df.loc[arima_mask, 'sales'] = (\n",
    "                ensemble_df.loc[arima_mask, 'sales'] * 0.5 + \n",
    "                ensemble_df.loc[arima_mask, 'sales_arima'] * 0.5\n",
    "            )\n",
    "        \n",
    "        final_submission = ensemble_df[['id', 'sales']].copy()\n",
    "        \n",
    "    else:\n",
    "        final_submission = sf_pred_df.copy()\n",
    "    \n",
    "    # Handle missing predictions\n",
    "    all_test_ids = set(test_fe['id'])\n",
    "    predicted_ids = set(final_submission['id'])\n",
    "    missing_ids = all_test_ids - predicted_ids\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\">>> Filling {len(missing_ids)} missing predictions...\")\n",
    "        median_sales = train_fe['sales'].median()\n",
    "        missing_df = pd.DataFrame({'id': list(missing_ids), 'sales': [median_sales] * len(missing_ids)})\n",
    "        final_submission = pd.concat([final_submission, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Final cleanup\n",
    "    final_submission['sales'] = final_submission['sales'].fillna(train_fe['sales'].median())\n",
    "    final_submission['sales'] = np.maximum(final_submission['sales'], 0.01)\n",
    "    final_submission = final_submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "else:\n",
    "    print(\">>> ERROR: No predictions generated!\")\n",
    "\n",
    "# ============================\n",
    "# 9️⃣ Calculate RMSLE and create evaluation report (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "def calculate_rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Logarithmic Error as per competition formula\"\"\"\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    log_diff = np.log(1 + y_pred) - np.log(1 + y_true)\n",
    "    rmsle = np.sqrt(np.mean(log_diff ** 2))\n",
    "    return rmsle\n",
    "\n",
    "# Merge predictions with actual values\n",
    "test_results = test_fe[['id', 'date', 'store_nbr', 'family', 'sales']].copy()\n",
    "test_results.columns = ['id', 'date', 'store_nbr', 'family', 'actual_sales']\n",
    "test_results = test_results.merge(final_submission[['id', 'sales']], on='id', how='left')\n",
    "test_results.columns = ['id', 'date', 'store_nbr', 'family', 'actual_sales', 'predicted_sales']\n",
    "\n",
    "# Calculate RMSLE\n",
    "rmsle = calculate_rmsle(test_results['actual_sales'], test_results['predicted_sales'])\n",
    "rmsle_sklearn = np.sqrt(mean_squared_log_error(test_results['actual_sales'], test_results['predicted_sales']))\n",
    "\n",
    "# ============================\n",
    "# 🔟 Create visualizations (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(\">>> Creating visualizations...\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(f'Enhanced RF + ARIMA Ensemble Results (RMSLE: {rmsle:.4f})', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Overall Actual vs Predicted\n",
    "axes[0, 0].scatter(test_results['actual_sales'], test_results['predicted_sales'], alpha=0.6, s=20)\n",
    "axes[0, 0].plot([0, test_results['actual_sales'].max()], [0, test_results['actual_sales'].max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Sales')\n",
    "axes[0, 0].set_ylabel('Predicted Sales')\n",
    "axes[0, 0].set_title('Actual vs Predicted Sales')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Time series for top stores\n",
    "top_stores = test_results.groupby('store_nbr')['actual_sales'].sum().nlargest(5).index\n",
    "for i, store in enumerate(top_stores[:3]):\n",
    "    store_data = test_results[test_results['store_nbr'] == store].groupby('date').agg({\n",
    "        'actual_sales': 'sum',\n",
    "        'predicted_sales': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    axes[0, 1].plot(store_data['date'], store_data['actual_sales'], \n",
    "                   label=f'Store {store} Actual', linestyle='-', alpha=0.8)\n",
    "    axes[0, 1].plot(store_data['date'], store_data['predicted_sales'], \n",
    "                   label=f'Store {store} Predicted', linestyle='--', alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Sales')\n",
    "axes[0, 1].set_title('Time Series: Top 3 Stores')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Error distribution\n",
    "residuals = test_results['actual_sales'] - test_results['predicted_sales']\n",
    "axes[1, 0].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(residuals.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[1, 0].axvline(0, color='green', linestyle='-', alpha=0.8, label='Perfect Prediction')\n",
    "axes[1, 0].set_xlabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Prediction Error Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Top families performance\n",
    "top_families = test_results.groupby('family')['actual_sales'].sum().nlargest(8).index\n",
    "family_performance = []\n",
    "\n",
    "for family in top_families:\n",
    "    family_data = test_results[test_results['family'] == family]\n",
    "    family_rmsle = calculate_rmsle(family_data['actual_sales'], family_data['predicted_sales'])\n",
    "    total_sales = family_data['actual_sales'].sum()\n",
    "    family_performance.append((family, family_rmsle, total_sales))\n",
    "\n",
    "family_df_perf = pd.DataFrame(family_performance, columns=['Family', 'RMSLE', 'Total_Sales'])\n",
    "family_df_perf = family_df_perf.sort_values('RMSLE')\n",
    "\n",
    "bars = axes[1, 1].bar(range(len(family_df_perf)), family_df_perf['RMSLE'], \n",
    "                     color=plt.cm.viridis(np.linspace(0, 1, len(family_df_perf))))\n",
    "axes[1, 1].set_xlabel('Product Family')\n",
    "axes[1, 1].set_ylabel('RMSLE')\n",
    "axes[1, 1].set_title('RMSLE by Top Product Families')\n",
    "axes[1, 1].set_xticks(range(len(family_df_perf)))\n",
    "axes[1, 1].set_xticklabels([f.replace('_', '\\n') for f in family_df_perf['Family']], \n",
    "                          rotation=45, ha='right', fontsize=8)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 11️⃣ Feature importance visualization (ADDED AS REQUESTED)\n",
    "# ============================\n",
    "if rf_predictions is not None:\n",
    "    print(\">>> Creating feature importance visualization...\")\n",
    "    \n",
    "    fig2, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    top_20_features = importance_df.head(20)\n",
    "    bars = ax.barh(range(len(top_20_features)), top_20_features['importance'])\n",
    "    ax.set_yticks(range(len(top_20_features)))\n",
    "    ax.set_yticklabels(top_20_features['feature'], fontsize=10)\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title('Random Forest Feature Importance (Top 20)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================\n",
    "# 12️⃣ Final evaluation report (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED RF+ARIMA ENSEMBLE TEST RESULTS (LOG TRANSFORM)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n>>> COMPETITION EVALUATION METRIC (RMSLE):\")\n",
    "print(f\">>> Root Mean Squared Logarithmic Error: {rmsle:.6f}\")\n",
    "print(f\">>> RMSLE (sklearn verification): {rmsle_sklearn:.6f}\")\n",
    "\n",
    "print(f\"\\n>>> SUBMISSION DETAILS:\")\n",
    "print(f\">>> Test predictions shape: {final_submission.shape}\")\n",
    "print(f\">>> Sales range: {final_submission['sales'].min():.3f} - {final_submission['sales'].max():.3f}\")\n",
    "print(f\">>> Sales median: {final_submission['sales'].median():.3f}\")\n",
    "print(f\">>> Sales mean: {final_submission['sales'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\n>>> MODEL COVERAGE:\")\n",
    "print(f\">>> Random Forest coverage: 100.0%\")\n",
    "print(f\">>> ARIMA coverage: {len(arima_df) / len(final_submission) * 100:.1f}%\")\n",
    "print(f\">>> Store-family coverage: {len(sf_pred_df) / len(final_submission) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n>>> ENHANCED ENSEMBLE COMPOSITION (SAME AS MAIN.IPYNB):\")\n",
    "arima_coverage = len(arima_df) / len(final_submission) if len(final_submission) > 0 else 0\n",
    "print(f\">>> For {arima_coverage*100:.1f}% of predictions: 50% (RF+SF) + 50% ARIMA (INCREASED)\")\n",
    "print(f\">>> For remaining predictions: 55% RF + 45% Store-Family\")\n",
    "print(f\">>> Key improvements:\")\n",
    "print(f\"    - Random Forest trained on log-transformed sales (better for skewed data)\")\n",
    "print(f\"    - Reduced overfitting: max_depth=10, min_samples_split=20, min_samples_leaf=10\")\n",
    "print(f\"    - Added max_features='sqrt' and max_samples=0.8 for regularization\")\n",
    "print(f\"    - ARIMA coverage increased from 100 to 200 top combinations\")\n",
    "print(f\"    - ARIMA weight increased from 30% to 50% where available\")\n",
    "print(f\"    - All lag features computed in log space for consistency\")\n",
    "print(f\"    - Enhanced ARIMA with more parameter configurations\")\n",
    "\n",
    "print(f\"\\n>>> VALIDATION METRICS ON INTERNAL TEST SET:\")\n",
    "print(f\">>> Mean Absolute Error: {np.mean(np.abs(test_results['actual_sales'] - test_results['predicted_sales'])):.4f}\")\n",
    "print(f\">>> Mean Squared Error: {np.mean((test_results['actual_sales'] - test_results['predicted_sales'])**2):.4f}\")\n",
    "print(f\">>> Pearson Correlation: {np.corrcoef(test_results['actual_sales'], test_results['predicted_sales'])[0,1]:.4f}\")\n",
    "print(f\">>> R² Score: {1 - np.sum((test_results['actual_sales'] - test_results['predicted_sales'])**2) / np.sum((test_results['actual_sales'] - test_results['actual_sales'].mean())**2):.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 13️⃣ Detailed performance analysis (SAME AS MAIN.IPYNB)\n",
    "# ============================\n",
    "print(f\"\\n>>> DETAILED PERFORMANCE BREAKDOWN:\")\n",
    "print(\">>> DETAILED FAMILY PERFORMANCE:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "family_metrics = []\n",
    "for family in test_results['family'].unique():\n",
    "    family_data = test_results[test_results['family'] == family]\n",
    "    if len(family_data) > 0:\n",
    "        actual = family_data['actual_sales']\n",
    "        predicted = family_data['predicted_sales']\n",
    "        \n",
    "        family_rmsle = calculate_rmsle(actual, predicted)\n",
    "        family_mae = np.mean(np.abs(actual - predicted))\n",
    "        family_mape = np.mean(np.abs((actual - predicted) / (actual + 1))) * 100\n",
    "        family_corr = np.corrcoef(actual, predicted)[0,1] if len(actual) > 1 else 0\n",
    "        family_bias = np.mean(predicted - actual)\n",
    "        total_actual = actual.sum()\n",
    "        sample_count = len(family_data)\n",
    "        \n",
    "        family_metrics.append({\n",
    "            'Family': family,\n",
    "            'RMSLE': family_rmsle,\n",
    "            'MAE': family_mae,\n",
    "            'MAPE': family_mape,\n",
    "            'Correlation': family_corr,\n",
    "            'Bias': family_bias,\n",
    "            'Samples': sample_count,\n",
    "            'Total_Actual': total_actual\n",
    "        })\n",
    "\n",
    "family_metrics_df = pd.DataFrame(family_metrics)\n",
    "family_metrics_df = family_metrics_df.sort_values('RMSLE', ascending=False)\n",
    "\n",
    "print(f\"{'Family':<25} {'RMSLE':<8} {'MAE':<8} {'MAPE%':<8} {'Corr':<8} {'Bias':<8} {'Samples':<8}\")\n",
    "print(\"-\" * 100)\n",
    "for _, row in family_metrics_df.head(15).iterrows():\n",
    "    print(f\"{row['Family']:<25} {row['RMSLE']:<8.4f} {row['MAE']:<8.2f} {row['MAPE']:<8.2f} {row['Correlation']:<8.3f} {row['Bias']:<8.2f} {row['Samples']:<8}\")\n",
    "\n",
    "print(f\"\\n>>> WORST PERFORMING FAMILIES (Top 5 by RMSLE):\")\n",
    "for _, row in family_metrics_df.head(5).iterrows():\n",
    "    print(f\"  • {row['Family']}: RMSLE={row['RMSLE']:.4f}, Samples={row['Samples']}, Total_Volume={row['Total_Actual']:.0f}\")\n",
    "\n",
    "print(f\"\\n>>> BEST PERFORMING FAMILIES (Bottom 5 by RMSLE):\")\n",
    "for _, row in family_metrics_df.tail(5).iterrows():\n",
    "    print(f\"  • {row['Family']}: RMSLE={row['RMSLE']:.4f}, Samples={row['Samples']}, Total_Volume={row['Total_Actual']:.0f}\")\n",
    "\n",
    "print(f\"\\n>>> COMPETITION READINESS:\")\n",
    "if rmsle < 0.40:\n",
    "    print(f\">>> ✓ EXCELLENT: RMSLE {rmsle:.4f} < 0.40 (Target achieved!)\")\n",
    "elif rmsle < 0.45:\n",
    "    print(f\">>> ✓ GOOD: RMSLE {rmsle:.4f} < 0.45 (Close to target)\")\n",
    "elif rmsle < 0.50:\n",
    "    print(f\">>> ⚠ FAIR: RMSLE {rmsle:.4f} < 0.50 (Needs improvement)\")\n",
    "else:\n",
    "    print(f\">>> ✗ POOR: RMSLE {rmsle:.4f} >= 0.50 (Significant improvement needed)\")\n",
    "\n",
    "print(f\"\\n>>> KEY DIFFERENCES FROM ORIGINAL TEST SCRIPT:\")\n",
    "print(\">>> ✓ Added log transform for sales (log_sales = log1p(sales))\")\n",
    "print(\">>> ✓ Random Forest trained on log_sales target instead of raw sales\")\n",
    "print(\">>> ✓ All lag features computed in log space for consistency\")\n",
    "print(\">>> ✓ Reduced overfitting RF parameters (max_depth=10, min_samples_split=20)\")\n",
    "print(\">>> ✓ Added RF regularization (max_features='sqrt', max_samples=0.8)\")\n",
    "print(\">>> ✓ Enhanced ARIMA with more parameter configurations\")\n",
    "print(\">>> ✓ Increased ARIMA coverage (200 combinations vs 150)\")\n",
    "print(\">>> ✓ Increased ARIMA ensemble weight (50% vs 25%)\")\n",
    "print(\">>> ✓ Store/family statistics computed in log space\")\n",
    "print(\">>> ✓ Comprehensive feature importance analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED TEST EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b199088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7baa0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
