{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6329cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Train period: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      ">>> Test period: 2017-08-16 00:00:00 to 2017-08-31 00:00:00\n",
      ">>> Creating smart features...\n",
      ">>> Adding oil & promotion features...\n",
      ">>> Creating lag features...\n",
      ">>> Creating aggregate features...\n",
      ">>> Training Random Forest...\n",
      ">>> Using 41 features for Random Forest\n",
      ">>> Random Forest completed\n",
      ">>> Top 10 Random Forest features:\n",
      "          feature  importance\n",
      "       sales_ma_7    0.225452\n",
      "      sales_ma_14    0.172364\n",
      "      sales_lag_7    0.169633\n",
      "      sales_ma_28    0.093207\n",
      "      sales_lag_1    0.093170\n",
      "     sales_lag_14    0.087709\n",
      "     sales_std_14    0.053643\n",
      " family_sales_std    0.018108\n",
      "family_sales_mean    0.013506\n",
      "      onpromotion    0.011840\n",
      ">>> Running ARIMA with exogenous variables...\n",
      ">>> Processing ARIMA for 200 top store-family combinations...\n",
      ">>> ARIMA progress: 0/200\n",
      ">>> ARIMA progress: 50/200\n",
      ">>> ARIMA progress: 100/200\n",
      ">>> ARIMA progress: 150/200\n",
      ">>> ARIMA predictions generated for 3200 test samples\n",
      ">>> Creating enhanced store-family predictions...\n",
      ">>> Store-family progress: 0/1782\n",
      ">>> Store-family progress: 200/1782\n",
      ">>> Store-family progress: 400/1782\n",
      ">>> Store-family progress: 600/1782\n",
      ">>> Store-family progress: 800/1782\n",
      ">>> Store-family progress: 1000/1782\n",
      ">>> Store-family progress: 1200/1782\n",
      ">>> Store-family progress: 1400/1782\n",
      ">>> Store-family progress: 1600/1782\n",
      ">>> Creating ensemble predictions...\n",
      "\n",
      ">>> ENSEMBLE RESULTS:\n",
      ">>> Submission shape: (28512, 2)\n",
      ">>> Sales range: 0.071 - 17056.257\n",
      ">>> Sales median: 30.437\n",
      ">>> Random Forest coverage: 100%\n",
      ">>> ARIMA coverage: 11.2%\n",
      ">>> Store-family coverage: 100.0%\n",
      ">>> Submission saved as 'ensemble_submission_rf_arima.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# 0️⃣ Load & prepare data\n",
    "# ============================\n",
    "merged_train_df = pd.read_csv('merged_train_df.csv')\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "merged_train_df['date'] = pd.to_datetime(merged_train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "merged_train_df = merged_train_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "merged_train_df['store_nbr'] = merged_train_df['store_nbr'].astype(int)\n",
    "test_df['store_nbr'] = test_df['store_nbr'].astype(int)\n",
    "\n",
    "# Handle sales quality\n",
    "merged_train_df['sales'] = np.maximum(merged_train_df['sales'], 0.01)\n",
    "merged_train_df['log_sales'] = np.log1p(merged_train_df['sales'])\n",
    "\n",
    "print(f\">>> Train period: {merged_train_df['date'].min()} to {merged_train_df['date'].max()}\")\n",
    "print(f\">>> Test period: {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Smart Feature Engineering (Computationally Efficient)\n",
    "# ============================\n",
    "def create_smart_features(df):\n",
    "    \"\"\"Create high-impact features with minimal computation\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Essential time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['week'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # High-impact binary features\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | (df['day'] >= 28)).astype(int)\n",
    "    \n",
    "    # Critical cyclical encoding (top performers)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    # Days since start (trend)\n",
    "    start_date = pd.Timestamp('2013-01-01')\n",
    "    df['days_since_start'] = (df['date'] - start_date).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_oil_and_promo_features(df):\n",
    "    \"\"\"Add oil price and promotion features - key for Ecuador\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Oil price handling (critical for Ecuador retail)\n",
    "    if 'oil_price' in df.columns:\n",
    "        df['oil_price'] = df['oil_price'].fillna(method='ffill').fillna(method='bfill').fillna(100)\n",
    "    else:\n",
    "        df['oil_price'] = 100\n",
    "    \n",
    "    # Oil price transformations (most predictive)\n",
    "    df['oil_price_log'] = np.log1p(df['oil_price'])\n",
    "    oil_mean = df['oil_price'].mean()\n",
    "    df['oil_price_normalized'] = df['oil_price'] / oil_mean\n",
    "    df['oil_price_high'] = (df['oil_price'] > oil_mean * 1.1).astype(int)\n",
    "    df['oil_price_low'] = (df['oil_price'] < oil_mean * 0.9).astype(int)\n",
    "    \n",
    "    # Promotion features\n",
    "    if 'onpromotion' in df.columns:\n",
    "        df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['onpromotion'] = 0\n",
    "        \n",
    "    # Holiday features\n",
    "    if 'isHoliday' in df.columns:\n",
    "        df['isHoliday'] = df['isHoliday'].fillna(0).astype(int)\n",
    "        df['promo_holiday'] = df['onpromotion'] * df['isHoliday']\n",
    "    else:\n",
    "        df['isHoliday'] = 0\n",
    "        df['promo_holiday'] = 0\n",
    "    \n",
    "    # Special events\n",
    "    for col in ['earthquake_impact', 'salary_day_impact']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_efficient_lag_features(df_groups):\n",
    "    \"\"\"Create essential lag features efficiently\"\"\"\n",
    "    result_dfs = []\n",
    "    \n",
    "    for df_group in df_groups:\n",
    "        if len(df_group) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_group = df_group.copy().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Only most important lags (based on autocorrelation analysis)\n",
    "        df_group['sales_lag_1'] = df_group['sales'].shift(1)\n",
    "        df_group['sales_lag_7'] = df_group['sales'].shift(7)\n",
    "        df_group['sales_lag_14'] = df_group['sales'].shift(14)\n",
    "        \n",
    "        # Oil price lags (crucial for Ecuador)\n",
    "        df_group['oil_lag_7'] = df_group['oil_price'].shift(7)\n",
    "        df_group['oil_lag_14'] = df_group['oil_price'].shift(14)\n",
    "        \n",
    "        # Essential rolling features\n",
    "        df_group['sales_ma_7'] = df_group['sales'].rolling(7, min_periods=1).mean()\n",
    "        df_group['sales_ma_14'] = df_group['sales'].rolling(14, min_periods=1).mean()\n",
    "        df_group['sales_ma_28'] = df_group['sales'].rolling(28, min_periods=1).mean()\n",
    "        \n",
    "        # Volatility (key predictor)\n",
    "        df_group['sales_std_14'] = df_group['sales'].rolling(14, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Simple trend\n",
    "        df_group['trend_7_14'] = (df_group['sales_ma_7'] / (df_group['sales_ma_14'] + 0.01)).fillna(1.0)\n",
    "        df_group['trend_7_14'] = np.clip(df_group['trend_7_14'], 0.5, 2.0)\n",
    "        \n",
    "        # Fill missing lags with moving averages\n",
    "        df_group['sales_lag_1'] = df_group['sales_lag_1'].fillna(df_group['sales_ma_7'])\n",
    "        df_group['sales_lag_7'] = df_group['sales_lag_7'].fillna(df_group['sales_ma_14'])\n",
    "        df_group['sales_lag_14'] = df_group['sales_lag_14'].fillna(df_group['sales_ma_28'])\n",
    "        df_group['oil_lag_7'] = df_group['oil_lag_7'].fillna(df_group['oil_price'])\n",
    "        df_group['oil_lag_14'] = df_group['oil_lag_14'].fillna(df_group['oil_price'])\n",
    "        \n",
    "        result_dfs.append(df_group)\n",
    "    \n",
    "    return result_dfs\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\">>> Creating smart features...\")\n",
    "train_fe = create_smart_features(merged_train_df)\n",
    "test_fe = create_smart_features(test_df)\n",
    "\n",
    "print(\">>> Adding oil & promotion features...\")\n",
    "train_fe = add_oil_and_promo_features(train_fe)\n",
    "test_fe = add_oil_and_promo_features(test_fe)\n",
    "\n",
    "print(\">>> Creating lag features...\")\n",
    "# Process by store-family groups\n",
    "train_groups = [group for _, group in train_fe.groupby(['store_nbr', 'family'])]\n",
    "train_groups_with_lags = create_efficient_lag_features(train_groups)\n",
    "train_fe = pd.concat(train_groups_with_lags, ignore_index=True)\n",
    "train_fe = train_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Handle test lag features\n",
    "test_with_lags = []\n",
    "for (store, family), test_group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    test_group = test_group.copy().reset_index(drop=True)\n",
    "    \n",
    "    train_group = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_group) > 0:\n",
    "        # Use recent training values for initialization\n",
    "        recent_sales = train_group['sales'].tail(28)\n",
    "        recent_oil = train_group['oil_price'].tail(28)\n",
    "        \n",
    "        # Initialize lags\n",
    "        test_group['sales_lag_1'] = recent_sales.iloc[-1] if len(recent_sales) >= 1 else recent_sales.mean()\n",
    "        test_group['sales_lag_7'] = recent_sales.iloc[-7] if len(recent_sales) >= 7 else recent_sales.mean()\n",
    "        test_group['sales_lag_14'] = recent_sales.iloc[-14] if len(recent_sales) >= 14 else recent_sales.mean()\n",
    "        test_group['oil_lag_7'] = recent_oil.iloc[-7] if len(recent_oil) >= 7 else recent_oil.mean()\n",
    "        test_group['oil_lag_14'] = recent_oil.iloc[-14] if len(recent_oil) >= 14 else recent_oil.mean()\n",
    "        \n",
    "        # Rolling averages\n",
    "        test_group['sales_ma_7'] = recent_sales.tail(7).mean()\n",
    "        test_group['sales_ma_14'] = recent_sales.tail(14).mean()\n",
    "        test_group['sales_ma_28'] = recent_sales.mean()\n",
    "        test_group['sales_std_14'] = recent_sales.tail(14).std() if len(recent_sales) >= 14 else 0\n",
    "        test_group['trend_7_14'] = test_group['sales_ma_7'] / (test_group['sales_ma_14'] + 0.01)\n",
    "        \n",
    "    else:\n",
    "        # Default values\n",
    "        for col in ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_ma_7', 'sales_ma_14', 'sales_ma_28']:\n",
    "            test_group[col] = 1.0\n",
    "        test_group['oil_lag_7'] = test_group['oil_lag_14'] = 100.0\n",
    "        test_group['sales_std_14'] = 0.0\n",
    "        test_group['trend_7_14'] = 1.0\n",
    "    \n",
    "    test_with_lags.append(test_group)\n",
    "\n",
    "test_fe = pd.concat(test_with_lags, ignore_index=True)\n",
    "test_fe = test_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Aggregate Features (Efficient)\n",
    "# ============================\n",
    "print(\">>> Creating aggregate features...\")\n",
    "\n",
    "# Store-level statistics (key predictors)\n",
    "store_stats = train_fe.groupby('store_nbr')['sales'].agg(['mean', 'std']).reset_index()\n",
    "store_stats.columns = ['store_nbr', 'store_sales_mean', 'store_sales_std']\n",
    "store_stats['store_sales_std'] = store_stats['store_sales_std'].fillna(0)\n",
    "\n",
    "# Family-level statistics\n",
    "family_stats = train_fe.groupby('family')['sales'].agg(['mean', 'std']).reset_index()\n",
    "family_stats.columns = ['family', 'family_sales_mean', 'family_sales_std']\n",
    "family_stats['family_sales_std'] = family_stats['family_sales_std'].fillna(0)\n",
    "\n",
    "# Add to datasets\n",
    "train_fe = train_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "train_fe = train_fe.merge(family_stats, on='family', how='left')\n",
    "test_fe = test_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "test_fe = test_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Fill missing aggregates\n",
    "for col in ['store_sales_mean', 'store_sales_std', 'family_sales_mean', 'family_sales_std']:\n",
    "    overall_val = train_fe[col].median()\n",
    "    train_fe[col] = train_fe[col].fillna(overall_val)\n",
    "    test_fe[col] = test_fe[col].fillna(overall_val)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Encode Categoricals\n",
    "# ============================\n",
    "le_family = LabelEncoder()\n",
    "combined_families = pd.concat([train_fe['family'], test_fe['family']]).unique()\n",
    "le_family.fit(combined_families)\n",
    "train_fe['family_encoded'] = le_family.transform(train_fe['family'])\n",
    "test_fe['family_encoded'] = le_family.transform(test_fe['family'])\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Random Forest Model\n",
    "# ============================\n",
    "print(\">>> Training Random Forest...\")\n",
    "\n",
    "# Feature selection (most important based on previous analysis)\n",
    "rf_features = [\n",
    "    # Time features\n",
    "    'year', 'month', 'dayofweek', 'day', 'quarter', 'week',\n",
    "    'is_weekend', 'is_month_end', 'is_month_start', 'is_payday',\n",
    "    'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'days_since_start',\n",
    "    \n",
    "    # Oil features (crucial)\n",
    "    'oil_price', 'oil_price_log', 'oil_price_normalized', 'oil_price_high', 'oil_price_low',\n",
    "    'oil_lag_7', 'oil_lag_14',\n",
    "    \n",
    "    # Sales features\n",
    "    'sales_lag_1', 'sales_lag_7', 'sales_lag_14',\n",
    "    'sales_ma_7', 'sales_ma_14', 'sales_ma_28', 'sales_std_14', 'trend_7_14',\n",
    "    \n",
    "    # Promotion & Events\n",
    "    'onpromotion', 'isHoliday', 'promo_holiday', 'earthquake_impact', 'salary_day_impact',\n",
    "    \n",
    "    # Store & Category\n",
    "    'store_nbr', 'family_encoded',\n",
    "    'store_sales_mean', 'store_sales_std', 'family_sales_mean', 'family_sales_std'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_rf_features = [col for col in rf_features if col in train_fe.columns and col in test_fe.columns]\n",
    "print(f\">>> Using {len(available_rf_features)} features for Random Forest\")\n",
    "\n",
    "# Clean data\n",
    "train_clean = train_fe.dropna(subset=['sales_lag_1', 'sales_lag_7'])\n",
    "X_train_rf = train_clean[available_rf_features].fillna(0)\n",
    "y_train_rf = train_clean['sales']\n",
    "X_test_rf = test_fe[available_rf_features].fillna(0)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "rf_predictions = np.maximum(rf_model.predict(X_test_rf), 0.01)\n",
    "\n",
    "print(\">>> Random Forest completed\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_rf_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\">>> Top 10 Random Forest features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ ARIMA with Exogenous Variables (Efficient Implementation)\n",
    "# ============================\n",
    "print(\">>> Running ARIMA with exogenous variables...\")\n",
    "\n",
    "def fit_arima_with_exog(train_data, test_data, store_num, family_name):\n",
    "    \"\"\"Fit ARIMA model with exogenous variables efficiently\"\"\"\n",
    "    \n",
    "    if len(train_data) < 30:  # Need sufficient data\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        train_sorted = train_data.sort_values('date').tail(84)  # Last ~3 months\n",
    "        \n",
    "        # Endogenous variable (sales) \n",
    "        y = train_sorted['sales'].values\n",
    "        \n",
    "        # Exogenous variables (most important ones)\n",
    "        exog_cols = ['oil_price_normalized', 'onpromotion', 'isHoliday', 'is_weekend', \n",
    "                    'month_sin', 'month_cos', 'dow_sin', 'dow_cos']\n",
    "        \n",
    "        available_exog_cols = [col for col in exog_cols if col in train_sorted.columns]\n",
    "        \n",
    "        if len(available_exog_cols) == 0:\n",
    "            return None\n",
    "            \n",
    "        X_train = train_sorted[available_exog_cols].values\n",
    "        X_test = test_data[available_exog_cols].values\n",
    "        \n",
    "        # Simple ARIMA with exogenous variables\n",
    "        # Use SARIMAX for better handling of exogenous variables\n",
    "        model = SARIMAX(y, exog=X_train, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0),\n",
    "                       enforce_stationarity=False, enforce_invertibility=False)\n",
    "        \n",
    "        fitted_model = model.fit(disp=False, maxiter=50)\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = fitted_model.forecast(steps=len(test_data), exog=X_test)\n",
    "        forecast = np.maximum(forecast, 0.01)  # Ensure positive\n",
    "        \n",
    "        return forecast\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Run ARIMA for store-family combinations (sample to manage computation)\n",
    "arima_predictions = {}\n",
    "store_family_combinations = list(set(zip(train_fe['store_nbr'], train_fe['family'])))\n",
    "\n",
    "# Process top combinations by volume first\n",
    "combination_volumes = []\n",
    "for store, family in store_family_combinations:\n",
    "    vol = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]['sales'].sum()\n",
    "    combination_volumes.append((store, family, vol))\n",
    "\n",
    "# Sort by volume and take top combinations\n",
    "combination_volumes.sort(key=lambda x: x[2], reverse=True)\n",
    "top_combinations = [x[:2] for x in combination_volumes[:200]]  # Top 200 for efficiency\n",
    "\n",
    "print(f\">>> Processing ARIMA for {len(top_combinations)} top store-family combinations...\")\n",
    "\n",
    "arima_results = []\n",
    "for i, (store, family) in enumerate(top_combinations):\n",
    "    if i % 50 == 0:\n",
    "        print(f\">>> ARIMA progress: {i}/{len(top_combinations)}\")\n",
    "        \n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    test_subset = test_fe[(test_fe['store_nbr'] == store) & (test_fe['family'] == family)]\n",
    "    \n",
    "    if len(test_subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    arima_pred = fit_arima_with_exog(train_subset, test_subset, store, family)\n",
    "    \n",
    "    if arima_pred is not None:\n",
    "        result_df = test_subset[['id']].copy()\n",
    "        result_df['sales_arima'] = arima_pred\n",
    "        arima_results.append(result_df)\n",
    "\n",
    "# Combine ARIMA results\n",
    "if arima_results:\n",
    "    arima_df = pd.concat(arima_results, ignore_index=True)\n",
    "    print(f\">>> ARIMA predictions generated for {len(arima_df)} test samples\")\n",
    "else:\n",
    "    arima_df = pd.DataFrame(columns=['id', 'sales_arima'])\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Enhanced Store-Family Baseline\n",
    "# ============================\n",
    "print(\">>> Creating enhanced store-family predictions...\")\n",
    "\n",
    "sf_predictions = []\n",
    "all_combinations = set(zip(test_fe['store_nbr'], test_fe['family']))\n",
    "\n",
    "for i, (store, family) in enumerate(sorted(all_combinations)):\n",
    "    if i % 200 == 0:\n",
    "        print(f\">>> Store-family progress: {i}/{len(all_combinations)}\")\n",
    "        \n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    test_subset = test_fe[(test_fe['store_nbr'] == store) & (test_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_subset) < 5 or len(test_subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Enhanced prediction logic\n",
    "    recent_data = train_subset.sort_values('date').tail(42)  # Last 6 weeks\n",
    "    \n",
    "    predictions = []\n",
    "    for _, test_row in test_subset.iterrows():\n",
    "        # Base prediction\n",
    "        base_pred = recent_data['sales'].median()\n",
    "        \n",
    "        # Adjust for day of week seasonality\n",
    "        dow_sales = recent_data[recent_data['dayofweek'] == test_row['dayofweek']]['sales']\n",
    "        if len(dow_sales) >= 2:\n",
    "            dow_factor = dow_sales.mean() / (recent_data['sales'].mean() + 0.01)\n",
    "            dow_factor = np.clip(dow_factor, 0.7, 1.4)\n",
    "        else:\n",
    "            dow_factor = 1.0\n",
    "            \n",
    "        # Adjust for promotion\n",
    "        if test_row.get('onpromotion', 0) == 1:\n",
    "            promo_sales = recent_data[recent_data['onpromotion'] == 1]['sales']\n",
    "            normal_sales = recent_data[recent_data['onpromotion'] == 0]['sales']\n",
    "            if len(promo_sales) > 0 and len(normal_sales) > 0:\n",
    "                promo_factor = promo_sales.mean() / (normal_sales.mean() + 0.01)\n",
    "                promo_factor = np.clip(promo_factor, 1.0, 1.6)\n",
    "            else:\n",
    "                promo_factor = 1.1\n",
    "        else:\n",
    "            promo_factor = 1.0\n",
    "            \n",
    "        # Oil price effect\n",
    "        recent_oil = recent_data['oil_price'].mean()\n",
    "        current_oil = test_row.get('oil_price', recent_oil)\n",
    "        oil_factor = 1.0 - 0.1 * (current_oil - recent_oil) / (recent_oil + 0.01)  # Inverse relationship\n",
    "        oil_factor = np.clip(oil_factor, 0.8, 1.2)\n",
    "        \n",
    "        # Combine factors\n",
    "        final_pred = base_pred * dow_factor * promo_factor * oil_factor\n",
    "        predictions.append(max(final_pred, 0.01))\n",
    "    \n",
    "    # Store results\n",
    "    result_df = test_subset[['id']].copy()\n",
    "    result_df['sales_sf'] = predictions\n",
    "    sf_predictions.append(result_df)\n",
    "\n",
    "sf_df = pd.concat(sf_predictions, ignore_index=True) if sf_predictions else pd.DataFrame(columns=['id', 'sales_sf'])\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Ensemble Predictions\n",
    "# ============================\n",
    "print(\">>> Creating ensemble predictions...\")\n",
    "\n",
    "# Start with Random Forest predictions\n",
    "final_df = test_fe[['id']].copy()\n",
    "final_df['sales_rf'] = rf_predictions\n",
    "\n",
    "# Add ARIMA predictions where available\n",
    "if len(arima_df) > 0:\n",
    "    final_df = final_df.merge(arima_df[['id', 'sales_arima']], on='id', how='left')\n",
    "else:\n",
    "    final_df['sales_arima'] = np.nan\n",
    "\n",
    "# Add store-family predictions\n",
    "if len(sf_df) > 0:\n",
    "    final_df = final_df.merge(sf_df[['id', 'sales_sf']], on='id', how='left')\n",
    "else:\n",
    "    final_df['sales_sf'] = np.nan\n",
    "\n",
    "# Create ensemble with adaptive weights\n",
    "def create_ensemble(row):\n",
    "    \"\"\"Create ensemble prediction with adaptive weighting\"\"\"\n",
    "    rf_pred = row['sales_rf']\n",
    "    arima_pred = row['sales_arima']\n",
    "    sf_pred = row['sales_sf']\n",
    "    \n",
    "    predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    # Random Forest (always available)\n",
    "    predictions.append(rf_pred)\n",
    "    weights.append(0.5)  # Base weight\n",
    "    \n",
    "    # ARIMA (when available)\n",
    "    if not pd.isna(arima_pred):\n",
    "        predictions.append(arima_pred)\n",
    "        weights.append(0.3)\n",
    "        \n",
    "    # Store-family (when available)  \n",
    "    if not pd.isna(sf_pred):\n",
    "        predictions.append(sf_pred)\n",
    "        weights.append(0.2)\n",
    "    \n",
    "    # Weighted average\n",
    "    total_weight = sum(weights)\n",
    "    ensemble_pred = sum(p * w for p, w in zip(predictions, weights)) / total_weight\n",
    "    \n",
    "    return max(ensemble_pred, 0.01)\n",
    "\n",
    "final_df['sales'] = final_df.apply(create_ensemble, axis=1)\n",
    "\n",
    "# Final validation and cleanup\n",
    "final_df = final_df[['id', 'sales']].sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Ensure all test IDs are covered\n",
    "all_test_ids = set(test_fe['id'])\n",
    "predicted_ids = set(final_df['id'])\n",
    "missing_ids = all_test_ids - predicted_ids\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\">>> Adding {len(missing_ids)} missing predictions...\")\n",
    "    median_sales = train_fe['sales'].median()\n",
    "    missing_df = pd.DataFrame({'id': list(missing_ids), 'sales': [median_sales] * len(missing_ids)})\n",
    "    final_df = pd.concat([final_df, missing_df], ignore_index=True)\n",
    "    final_df = final_df.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Final statistics\n",
    "final_df['sales'] = final_df['sales'].fillna(train_fe['sales'].median())\n",
    "final_df['sales'] = np.maximum(final_df['sales'], 0.01)\n",
    "\n",
    "# Save submission\n",
    "final_df.to_csv(\"ensemble_submission_rf_arima.csv\", index=False)\n",
    "\n",
    "print(f\"\\n>>> ENSEMBLE RESULTS:\")\n",
    "print(f\">>> Submission shape: {final_df.shape}\")\n",
    "print(f\">>> Sales range: {final_df['sales'].min():.3f} - {final_df['sales'].max():.3f}\")\n",
    "print(f\">>> Sales median: {final_df['sales'].median():.3f}\")\n",
    "print(f\">>> Random Forest coverage: 100%\")\n",
    "print(f\">>> ARIMA coverage: {len(arima_df) / len(final_df) * 100:.1f}%\")\n",
    "print(f\">>> Store-family coverage: {len(sf_df) / len(final_df) * 100:.1f}%\")\n",
    "print(\">>> Submission saved as 'ensemble_submission_rf_arima.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d245dcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "sales                       float64\n",
       "onpromotion                   int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "isHoliday                     int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "onpromotion                   int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "isHoliday                     int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f133a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n",
      "Infinite values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Check for NaN\n",
    "print(\"NaN values per column:\\n\", submission.isna().sum())\n",
    "\n",
    "# Check for inf\n",
    "print(\"Infinite values per column:\\n\", np.isinf(submission).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb04ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, sales]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Find rows where sales is infinite\n",
    "mask = np.isinf(submission[\"sales\"])\n",
    "\n",
    "# Show the offending rows\n",
    "print(submission.loc[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b51f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Replace inf with NaN first (so ffill works)\n",
    "submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Forward fill\n",
    "submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# (Optional) if the very first row is NaN/inf, ffill won’t work — so backfill as fallback\n",
    "submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Save cleaned file\n",
    "submission.to_csv(\"my_submission_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42f2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Test set ID range: 3000888 to 3029399\n",
      "▶️ Submission ID range: 3000888 to 3029399\n",
      "✅ All submission IDs are inside test_df.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your files\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Check the range of IDs\n",
    "print(\"▶️ Test set ID range:\", test_df[\"id\"].min(), \"to\", test_df[\"id\"].max())\n",
    "print(\"▶️ Submission ID range:\", submission[\"id\"].min(), \"to\", submission[\"id\"].max())\n",
    "\n",
    "# Also check for IDs in submission but not in test\n",
    "extra_ids = set(submission[\"id\"]) - set(test_df[\"id\"])\n",
    "if extra_ids:\n",
    "    print(\"⚠️ IDs present in submission but not in test:\", list(extra_ids)[:10], \"...\")\n",
    "else:\n",
    "    print(\"✅ All submission IDs are inside test_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
