{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6329cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Train period: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      ">>> Test period: 2017-08-16 00:00:00 to 2017-08-31 00:00:00\n",
      ">>> Creating core features...\n",
      ">>> Creating lag features...\n",
      ">>> Creating aggregate features...\n",
      ">>> Training Random Forest...\n",
      ">>> Using 44 features for Random Forest\n",
      ">>> Random Forest completed\n",
      ">>> Top 10 Random Forest features:\n",
      "         feature  importance\n",
      "      sales_ma_7    0.706201\n",
      "     sales_ma_14    0.126933\n",
      "     sales_lag_7    0.079838\n",
      "    sales_lag_14    0.033890\n",
      "    sales_lag_28    0.010916\n",
      "     sales_lag_1    0.008019\n",
      "     sales_std_7    0.007630\n",
      "sales_trend_7_14    0.007249\n",
      "             day    0.002947\n",
      "    sales_std_14    0.002350\n",
      ">>> Running Simple ARIMA for key store-family combinations...\n",
      ">>> Selecting combinations for ARIMA...\n",
      ">>> Processing ARIMA for 150 top combinations...\n",
      ">>> ARIMA progress: 0/150\n",
      ">>> ARIMA progress: 50/150\n",
      ">>> ARIMA progress: 100/150\n",
      ">>> ARIMA predictions generated for 2400 test samples\n",
      ">>> Creating store-family baseline...\n",
      ">>> Creating three-model ensemble...\n",
      "\n",
      ">>> RF + ARIMA ENSEMBLE RESULTS:\n",
      ">>> Submission shape: (28512, 2)\n",
      ">>> Sales range: 0.010 - 5895.000\n",
      ">>> Sales median: 29.386\n",
      ">>> Sales mean: 412.963\n",
      ">>> Random Forest coverage: 100%\n",
      ">>> ARIMA coverage: 8.4%\n",
      ">>> Store-family coverage: 100.0%\n",
      "\n",
      ">>> ENSEMBLE WEIGHTS:\n",
      ">>> Random Forest: 60% (main predictor)\n",
      ">>> ARIMA: 25% (time series patterns)\n",
      ">>> Store-Family: 15% (local knowledge)\n",
      "\n",
      ">>> KEY FEATURES:\n",
      ">>> ✓ Focused feature set (~40 features)\n",
      ">>> ✓ Simple ARIMA with automatic order selection\n",
      ">>> ✓ Median-based store-family predictions\n",
      ">>> ✓ Three-model ensemble with fixed weights\n",
      ">>> ✓ Conservative outlier handling\n",
      ">>> ✓ Top 150 combinations for ARIMA efficiency\n",
      ">>> Submission saved as 'rf_arima_ensemble_submission.csv'\n",
      ">>> Target: < 0.40 RMSLE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# 0️⃣ Load & prepare data\n",
    "# ============================\n",
    "merged_train_df = pd.read_csv('merged_train_df.csv')\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "merged_train_df['date'] = pd.to_datetime(merged_train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "merged_train_df = merged_train_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "merged_train_df['store_nbr'] = merged_train_df['store_nbr'].astype(int)\n",
    "test_df['store_nbr'] = test_df['store_nbr'].astype(int)\n",
    "\n",
    "# Handle sales\n",
    "merged_train_df['sales'] = np.maximum(merged_train_df['sales'], 0.01)\n",
    "\n",
    "print(f\">>> Train period: {merged_train_df['date'].min()} to {merged_train_df['date'].max()}\")\n",
    "print(f\">>> Test period: {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Focused Feature Engineering\n",
    "# ============================\n",
    "def create_core_features(df):\n",
    "    \"\"\"Create essential features only\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Core time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # Key binary features\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | (df['day'] >= 28)).astype(int)\n",
    "    \n",
    "    # Essential cyclical features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    # Trend\n",
    "    start_date = pd.Timestamp('2013-01-01')\n",
    "    df['days_since_start'] = (df['date'] - start_date).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_external_features(df):\n",
    "    \"\"\"Add oil, promotion, and holiday features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Oil price\n",
    "    if 'oil_price' in df.columns:\n",
    "        df['oil_price'] = df['oil_price'].fillna(method='ffill').fillna(method='bfill').fillna(100)\n",
    "    else:\n",
    "        df['oil_price'] = 100\n",
    "    \n",
    "    # Oil price features\n",
    "    df['oil_price_norm'] = df['oil_price'] / df['oil_price'].mean()\n",
    "    df['oil_price_ma_7'] = df['oil_price'].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    # Promotion\n",
    "    if 'onpromotion' in df.columns:\n",
    "        df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['onpromotion'] = 0\n",
    "        \n",
    "    # Holiday\n",
    "    if 'isHoliday' in df.columns:\n",
    "        df['isHoliday'] = df['isHoliday'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['isHoliday'] = 0\n",
    "    \n",
    "    # Key interactions\n",
    "    df['promo_weekend'] = df['onpromotion'] * df['is_weekend']\n",
    "    df['promo_holiday'] = df['onpromotion'] * df['isHoliday']\n",
    "    \n",
    "    # Handle special events if they exist\n",
    "    for col in ['earthquake_impact', 'salary_day_impact']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df_groups):\n",
    "    \"\"\"Create essential lag and rolling features\"\"\"\n",
    "    result_dfs = []\n",
    "    \n",
    "    for df_group in df_groups:\n",
    "        if len(df_group) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_group = df_group.copy().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Key lag features\n",
    "        df_group['sales_lag_1'] = df_group['sales'].shift(1)\n",
    "        df_group['sales_lag_7'] = df_group['sales'].shift(7)\n",
    "        df_group['sales_lag_14'] = df_group['sales'].shift(14)\n",
    "        df_group['sales_lag_28'] = df_group['sales'].shift(28)\n",
    "        \n",
    "        # Rolling averages\n",
    "        df_group['sales_ma_7'] = df_group['sales'].rolling(7, min_periods=1).mean()\n",
    "        df_group['sales_ma_14'] = df_group['sales'].rolling(14, min_periods=1).mean()\n",
    "        df_group['sales_ma_28'] = df_group['sales'].rolling(28, min_periods=1).mean()\n",
    "        \n",
    "        # Rolling std\n",
    "        df_group['sales_std_7'] = df_group['sales'].rolling(7, min_periods=1).std().fillna(0)\n",
    "        df_group['sales_std_14'] = df_group['sales'].rolling(14, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Trend indicators\n",
    "        df_group['sales_trend_7_14'] = (df_group['sales_ma_7'] / (df_group['sales_ma_14'] + 0.01)).fillna(1.0)\n",
    "        df_group['sales_trend_14_28'] = (df_group['sales_ma_14'] / (df_group['sales_ma_28'] + 0.01)).fillna(1.0)\n",
    "        \n",
    "        # Oil lags\n",
    "        df_group['oil_lag_7'] = df_group['oil_price'].shift(7)\n",
    "        df_group['oil_lag_14'] = df_group['oil_price'].shift(14)\n",
    "        \n",
    "        # Promotion lags\n",
    "        df_group['promo_lag_7'] = df_group['onpromotion'].shift(7)\n",
    "        \n",
    "        # Fill missing lags\n",
    "        df_group['sales_lag_1'] = df_group['sales_lag_1'].fillna(df_group['sales_ma_7'])\n",
    "        df_group['sales_lag_7'] = df_group['sales_lag_7'].fillna(df_group['sales_ma_14'])\n",
    "        df_group['sales_lag_14'] = df_group['sales_lag_14'].fillna(df_group['sales_ma_28'])\n",
    "        df_group['sales_lag_28'] = df_group['sales_lag_28'].fillna(df_group['sales_ma_28'])\n",
    "        \n",
    "        df_group['oil_lag_7'] = df_group['oil_lag_7'].fillna(df_group['oil_price'])\n",
    "        df_group['oil_lag_14'] = df_group['oil_lag_14'].fillna(df_group['oil_price'])\n",
    "        df_group['promo_lag_7'] = df_group['promo_lag_7'].fillna(0)\n",
    "        \n",
    "        # Clip trend features\n",
    "        df_group['sales_trend_7_14'] = np.clip(df_group['sales_trend_7_14'], 0.5, 2.0)\n",
    "        df_group['sales_trend_14_28'] = np.clip(df_group['sales_trend_14_28'], 0.5, 2.0)\n",
    "        \n",
    "        result_dfs.append(df_group)\n",
    "    \n",
    "    return result_dfs\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\">>> Creating core features...\")\n",
    "train_fe = create_core_features(merged_train_df)\n",
    "test_fe = create_core_features(test_df)\n",
    "\n",
    "train_fe = add_external_features(train_fe)\n",
    "test_fe = add_external_features(test_fe)\n",
    "\n",
    "print(\">>> Creating lag features...\")\n",
    "train_groups = [group for _, group in train_fe.groupby(['store_nbr', 'family'])]\n",
    "train_groups_with_lags = create_lag_features(train_groups)\n",
    "train_fe = pd.concat(train_groups_with_lags, ignore_index=True)\n",
    "train_fe = train_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Initialize test lag features\n",
    "test_with_lags = []\n",
    "for (store, family), test_group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    test_group = test_group.copy().reset_index(drop=True)\n",
    "    \n",
    "    train_group = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_group) > 0:\n",
    "        recent_sales = train_group['sales'].tail(56)  # 8 weeks\n",
    "        recent_oil = train_group['oil_price'].tail(56)\n",
    "        recent_promo = train_group['onpromotion'].tail(56)\n",
    "        \n",
    "        # Initialize lags\n",
    "        test_group['sales_lag_1'] = recent_sales.iloc[-1] if len(recent_sales) >= 1 else recent_sales.mean()\n",
    "        test_group['sales_lag_7'] = recent_sales.iloc[-7] if len(recent_sales) >= 7 else recent_sales.mean()\n",
    "        test_group['sales_lag_14'] = recent_sales.iloc[-14] if len(recent_sales) >= 14 else recent_sales.mean()\n",
    "        test_group['sales_lag_28'] = recent_sales.iloc[-28] if len(recent_sales) >= 28 else recent_sales.mean()\n",
    "        \n",
    "        # Rolling averages\n",
    "        test_group['sales_ma_7'] = recent_sales.tail(7).mean()\n",
    "        test_group['sales_ma_14'] = recent_sales.tail(14).mean()\n",
    "        test_group['sales_ma_28'] = recent_sales.mean()\n",
    "        \n",
    "        # Rolling std\n",
    "        test_group['sales_std_7'] = recent_sales.tail(7).std() if len(recent_sales) >= 7 else 0\n",
    "        test_group['sales_std_14'] = recent_sales.tail(14).std() if len(recent_sales) >= 14 else 0\n",
    "        \n",
    "        # Trends\n",
    "        test_group['sales_trend_7_14'] = test_group['sales_ma_7'] / (test_group['sales_ma_14'] + 0.01)\n",
    "        test_group['sales_trend_14_28'] = test_group['sales_ma_14'] / (test_group['sales_ma_28'] + 0.01)\n",
    "        \n",
    "        # Oil lags\n",
    "        test_group['oil_lag_7'] = recent_oil.iloc[-7] if len(recent_oil) >= 7 else recent_oil.mean()\n",
    "        test_group['oil_lag_14'] = recent_oil.iloc[-14] if len(recent_oil) >= 14 else recent_oil.mean()\n",
    "        \n",
    "        # Promo lag\n",
    "        test_group['promo_lag_7'] = recent_promo.iloc[-7] if len(recent_promo) >= 7 else recent_promo.mean()\n",
    "        \n",
    "    else:\n",
    "        # Default values\n",
    "        default_sales = 1.0\n",
    "        for col in ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_28', \n",
    "                   'sales_ma_7', 'sales_ma_14', 'sales_ma_28']:\n",
    "            test_group[col] = default_sales\n",
    "        test_group['sales_std_7'] = test_group['sales_std_14'] = 0.0\n",
    "        test_group['sales_trend_7_14'] = test_group['sales_trend_14_28'] = 1.0\n",
    "        test_group['oil_lag_7'] = test_group['oil_lag_14'] = 100.0\n",
    "        test_group['promo_lag_7'] = 0.0\n",
    "    \n",
    "    test_with_lags.append(test_group)\n",
    "\n",
    "test_fe = pd.concat(test_with_lags, ignore_index=True)\n",
    "test_fe = test_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Store and Family Statistics\n",
    "# ============================\n",
    "print(\">>> Creating aggregate features...\")\n",
    "\n",
    "# Store stats\n",
    "store_stats = train_fe.groupby('store_nbr')['sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "store_stats.columns = ['store_nbr', 'store_sales_mean', 'store_sales_std', 'store_sales_median']\n",
    "store_stats['store_sales_std'] = store_stats['store_sales_std'].fillna(0)\n",
    "\n",
    "# Family stats  \n",
    "family_stats = train_fe.groupby('family')['sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "family_stats.columns = ['family', 'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "family_stats['family_sales_std'] = family_stats['family_sales_std'].fillna(0)\n",
    "\n",
    "# Add to datasets\n",
    "train_fe = train_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "train_fe = train_fe.merge(family_stats, on='family', how='left')\n",
    "test_fe = test_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "test_fe = test_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Fill missing\n",
    "for col in ['store_sales_mean', 'store_sales_std', 'store_sales_median', \n",
    "           'family_sales_mean', 'family_sales_std', 'family_sales_median']:\n",
    "    overall_val = train_fe[col].median()\n",
    "    train_fe[col] = train_fe[col].fillna(overall_val)\n",
    "    test_fe[col] = test_fe[col].fillna(overall_val)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Encoding\n",
    "# ============================\n",
    "le_family = LabelEncoder()\n",
    "combined_families = pd.concat([train_fe['family'], test_fe['family']]).unique()\n",
    "le_family.fit(combined_families)\n",
    "train_fe['family_encoded'] = le_family.transform(train_fe['family'])\n",
    "test_fe['family_encoded'] = le_family.transform(test_fe['family'])\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Random Forest Model\n",
    "# ============================\n",
    "print(\">>> Training Random Forest...\")\n",
    "\n",
    "# Core feature set\n",
    "rf_features = [\n",
    "    # Time features\n",
    "    'year', 'month', 'dayofweek', 'day', 'quarter',\n",
    "    'is_weekend', 'is_month_end', 'is_payday',\n",
    "    'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'days_since_start',\n",
    "    \n",
    "    # External features\n",
    "    'oil_price', 'oil_price_norm', 'oil_price_ma_7', 'onpromotion', 'isHoliday',\n",
    "    'promo_weekend', 'promo_holiday', 'earthquake_impact', 'salary_day_impact',\n",
    "    \n",
    "    # Lag features\n",
    "    'sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_28',\n",
    "    'sales_ma_7', 'sales_ma_14', 'sales_ma_28',\n",
    "    'sales_std_7', 'sales_std_14',\n",
    "    'sales_trend_7_14', 'sales_trend_14_28',\n",
    "    'oil_lag_7', 'oil_lag_14', 'promo_lag_7',\n",
    "    \n",
    "    # Store & Category\n",
    "    'store_nbr', 'family_encoded',\n",
    "    'store_sales_mean', 'store_sales_std', 'store_sales_median',\n",
    "    'family_sales_mean', 'family_sales_std', 'family_sales_median'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_rf_features = [col for col in rf_features if col in train_fe.columns and col in test_fe.columns]\n",
    "print(f\">>> Using {len(available_rf_features)} features for Random Forest\")\n",
    "\n",
    "# Prepare data\n",
    "train_clean = train_fe.dropna(subset=['sales_lag_1', 'sales_lag_7'])\n",
    "X_train_rf = train_clean[available_rf_features].fillna(0)\n",
    "y_train_rf = train_clean['sales']\n",
    "X_test_rf = test_fe[available_rf_features].fillna(0)\n",
    "\n",
    "# Train Random Forest with optimized parameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=120,\n",
    "    max_depth=12,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=3,\n",
    "    max_features=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "rf_predictions = np.maximum(rf_model.predict(X_test_rf), 0.01)\n",
    "\n",
    "print(\">>> Random Forest completed\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_rf_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\">>> Top 10 Random Forest features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Simple ARIMA Model\n",
    "# ============================\n",
    "print(\">>> Running Simple ARIMA for key store-family combinations...\")\n",
    "\n",
    "def fit_simple_arima(train_data, forecast_periods):\n",
    "    \"\"\"Simple ARIMA model focused on core patterns\"\"\"\n",
    "    \n",
    "    if len(train_data) < 28:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use recent data for better forecasting\n",
    "        recent_data = train_data.sort_values('date').tail(90)  # ~3 months\n",
    "        sales_series = recent_data['sales'].values\n",
    "        \n",
    "        # Simple differencing check\n",
    "        if len(sales_series) > 7:\n",
    "            # Check if series needs differencing\n",
    "            diff_series = np.diff(sales_series)\n",
    "            if np.std(diff_series) < np.std(sales_series):\n",
    "                d = 1\n",
    "            else:\n",
    "                d = 0\n",
    "        else:\n",
    "            d = 1\n",
    "        \n",
    "        # Try different ARIMA configurations\n",
    "        arima_configs = [\n",
    "            (1, d, 1),\n",
    "            (2, d, 1), \n",
    "            (1, d, 2),\n",
    "            (2, d, 2),\n",
    "            (0, d, 1),\n",
    "            (1, d, 0)\n",
    "        ]\n",
    "        \n",
    "        best_model = None\n",
    "        best_aic = float('inf')\n",
    "        \n",
    "        for p, d, q in arima_configs:\n",
    "            try:\n",
    "                model = ARIMA(sales_series, order=(p, d, q))\n",
    "                fitted_model = model.fit()\n",
    "                \n",
    "                if fitted_model.aic < best_aic:\n",
    "                    best_aic = fitted_model.aic\n",
    "                    best_model = fitted_model\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_model is not None:\n",
    "            # Forecast\n",
    "            forecast = best_model.forecast(steps=forecast_periods)\n",
    "            forecast = np.maximum(forecast, 0.01)  # Ensure positive\n",
    "            return forecast\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Select top store-family combinations by volume and consistency\n",
    "print(\">>> Selecting combinations for ARIMA...\")\n",
    "combination_scores = []\n",
    "\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    if len(group) >= 28:  # Minimum data requirement\n",
    "        volume = group['sales'].sum()\n",
    "        consistency = 1.0 / (group['sales'].std() / (group['sales'].mean() + 0.01) + 0.01)  # Inverse CV\n",
    "        data_points = len(group)\n",
    "        \n",
    "        # Composite score favoring high volume, consistent, well-sampled series\n",
    "        score = volume * consistency * np.log(data_points)\n",
    "        combination_scores.append((store, family, score))\n",
    "\n",
    "# Sort by score and take top combinations\n",
    "combination_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "top_combinations = [x[:2] for x in combination_scores[:150]]  # Top 150 for efficiency\n",
    "\n",
    "print(f\">>> Processing ARIMA for {len(top_combinations)} top combinations...\")\n",
    "\n",
    "arima_results = []\n",
    "for i, (store, family) in enumerate(top_combinations):\n",
    "    if i % 50 == 0:\n",
    "        print(f\">>> ARIMA progress: {i}/{len(top_combinations)}\")\n",
    "        \n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    test_subset = test_fe[(test_fe['store_nbr'] == store) & (test_fe['family'] == family)]\n",
    "    \n",
    "    if len(test_subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    arima_pred = fit_simple_arima(train_subset, len(test_subset))\n",
    "    \n",
    "    if arima_pred is not None:\n",
    "        result_df = test_subset[['id']].copy()\n",
    "        result_df['sales_arima'] = arima_pred\n",
    "        arima_results.append(result_df)\n",
    "\n",
    "# Combine ARIMA results\n",
    "if arima_results:\n",
    "    arima_df = pd.concat(arima_results, ignore_index=True)\n",
    "    print(f\">>> ARIMA predictions generated for {len(arima_df)} test samples\")\n",
    "else:\n",
    "    arima_df = pd.DataFrame(columns=['id', 'sales_arima'])\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Simple Store-Family Baseline\n",
    "# ============================\n",
    "print(\">>> Creating store-family baseline...\")\n",
    "\n",
    "def simple_sf_prediction(train_subset, test_row):\n",
    "    \"\"\"Simple but effective store-family prediction\"\"\"\n",
    "    \n",
    "    if len(train_subset) < 3:\n",
    "        return 1.0\n",
    "    \n",
    "    recent_data = train_subset.sort_values('date').tail(42)  # 6 weeks\n",
    "    \n",
    "    # Base prediction\n",
    "    base_pred = recent_data['sales'].median()\n",
    "    \n",
    "    # Day of week adjustment\n",
    "    dow_factor = 1.0\n",
    "    dow_sales = recent_data[recent_data['dayofweek'] == test_row['dayofweek']]['sales']\n",
    "    if len(dow_sales) >= 2:\n",
    "        dow_factor = dow_sales.median() / (recent_data['sales'].median() + 0.01)\n",
    "        dow_factor = np.clip(dow_factor, 0.7, 1.4)\n",
    "    \n",
    "    # Promotion adjustment\n",
    "    promo_factor = 1.0\n",
    "    if test_row.get('onpromotion', 0) == 1:\n",
    "        promo_sales = recent_data[recent_data['onpromotion'] == 1]['sales']\n",
    "        normal_sales = recent_data[recent_data['onpromotion'] == 0]['sales']\n",
    "        if len(promo_sales) >= 1 and len(normal_sales) >= 1:\n",
    "            promo_factor = promo_sales.median() / (normal_sales.median() + 0.01)\n",
    "            promo_factor = np.clip(promo_factor, 1.0, 1.6)\n",
    "        else:\n",
    "            promo_factor = 1.15\n",
    "    \n",
    "    final_pred = base_pred * dow_factor * promo_factor\n",
    "    return max(final_pred, 0.01)\n",
    "\n",
    "# Create store-family predictions\n",
    "sf_predictions = []\n",
    "for (store, family), test_group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_subset) < 2:\n",
    "        continue\n",
    "        \n",
    "    predictions = []\n",
    "    for _, test_row in test_group.iterrows():\n",
    "        pred = simple_sf_prediction(train_subset, test_row)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    if predictions:\n",
    "        result_df = test_group[['id']].copy()\n",
    "        result_df['sales_sf'] = predictions\n",
    "        sf_predictions.append(result_df)\n",
    "\n",
    "sf_df = pd.concat(sf_predictions, ignore_index=True) if sf_predictions else pd.DataFrame(columns=['id', 'sales_sf'])\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Three-Model Ensemble\n",
    "# ============================\n",
    "print(\">>> Creating three-model ensemble...\")\n",
    "\n",
    "# Combine all predictions\n",
    "final_df = test_fe[['id']].copy()\n",
    "final_df['sales_rf'] = rf_predictions\n",
    "\n",
    "# Add ARIMA predictions\n",
    "if len(arima_df) > 0:\n",
    "    final_df = final_df.merge(arima_df[['id', 'sales_arima']], on='id', how='left')\n",
    "else:\n",
    "    final_df['sales_arima'] = np.nan\n",
    "\n",
    "# Add store-family predictions  \n",
    "if len(sf_df) > 0:\n",
    "    final_df = final_df.merge(sf_df[['id', 'sales_sf']], on='id', how='left')\n",
    "else:\n",
    "    final_df['sales_sf'] = np.nan\n",
    "\n",
    "def three_model_ensemble(row):\n",
    "    \"\"\"Ensemble of Random Forest, ARIMA, and Store-Family predictions\"\"\"\n",
    "    \n",
    "    rf_pred = row['sales_rf']\n",
    "    arima_pred = row['sales_arima']\n",
    "    sf_pred = row['sales_sf']\n",
    "    \n",
    "    predictions = [rf_pred]  # RF always available\n",
    "    weights = [0.6]  # RF gets highest weight\n",
    "    \n",
    "    # Add ARIMA if available (good for trend/seasonality)\n",
    "    if not pd.isna(arima_pred):\n",
    "        predictions.append(arima_pred)\n",
    "        weights.append(0.25)\n",
    "    \n",
    "    # Add store-family if available (good for local patterns)\n",
    "    if not pd.isna(sf_pred):\n",
    "        predictions.append(sf_pred)\n",
    "        weights.append(0.15)\n",
    "    \n",
    "    # Weighted average\n",
    "    total_weight = sum(weights)\n",
    "    ensemble_pred = sum(p * w for p, w in zip(predictions, weights)) / total_weight\n",
    "    \n",
    "    return max(ensemble_pred, 0.01)\n",
    "\n",
    "final_df['sales'] = final_df.apply(three_model_ensemble, axis=1)\n",
    "\n",
    "# ============================\n",
    "# 8️⃣ Final Processing\n",
    "# ============================\n",
    "final_df = final_df[['id', 'sales']].sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Handle missing IDs\n",
    "all_test_ids = set(test_fe['id'])\n",
    "predicted_ids = set(final_df['id'])\n",
    "missing_ids = all_test_ids - predicted_ids\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\">>> Adding {len(missing_ids)} missing predictions...\")\n",
    "    median_sales = train_fe['sales'].median()\n",
    "    missing_df = pd.DataFrame({'id': list(missing_ids), 'sales': [median_sales] * len(missing_ids)})\n",
    "    final_df = pd.concat([final_df, missing_df], ignore_index=True)\n",
    "    final_df = final_df.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Final validation\n",
    "final_df['sales'] = final_df['sales'].fillna(train_fe['sales'].median())\n",
    "final_df['sales'] = np.maximum(final_df['sales'], 0.01)\n",
    "\n",
    "# Conservative outlier capping\n",
    "q95 = train_fe['sales'].quantile(0.95)\n",
    "final_df['sales'] = np.minimum(final_df['sales'], q95 * 3)\n",
    "\n",
    "# Save submission\n",
    "final_df.to_csv(\"rf_arima_ensemble_submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\n>>> RF + ARIMA ENSEMBLE RESULTS:\")\n",
    "print(f\">>> Submission shape: {final_df.shape}\")\n",
    "print(f\">>> Sales range: {final_df['sales'].min():.3f} - {final_df['sales'].max():.3f}\")\n",
    "print(f\">>> Sales median: {final_df['sales'].median():.3f}\")\n",
    "print(f\">>> Sales mean: {final_df['sales'].mean():.3f}\")\n",
    "print(f\">>> Random Forest coverage: 100%\")\n",
    "print(f\">>> ARIMA coverage: {len(arima_df) / len(final_df) * 100:.1f}%\")\n",
    "print(f\">>> Store-family coverage: {len(sf_df) / len(final_df) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n>>> ENSEMBLE WEIGHTS:\")\n",
    "print(\">>> Random Forest: 60% (main predictor)\")\n",
    "print(\">>> ARIMA: 25% (time series patterns)\")\n",
    "print(\">>> Store-Family: 15% (local knowledge)\")\n",
    "\n",
    "print(f\"\\n>>> KEY FEATURES:\")\n",
    "print(\">>> ✓ Focused feature set (~40 features)\")\n",
    "print(\">>> ✓ Simple ARIMA with automatic order selection\") \n",
    "print(\">>> ✓ Median-based store-family predictions\")\n",
    "print(\">>> ✓ Three-model ensemble with fixed weights\")\n",
    "print(\">>> ✓ Conservative outlier handling\")\n",
    "print(\">>> ✓ Top 150 combinations for ARIMA efficiency\")\n",
    "\n",
    "print(\">>> Submission saved as 'rf_arima_ensemble_submission.csv'\")\n",
    "print(\">>> Target: < 0.40 RMSLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d245dcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "sales                       float64\n",
       "onpromotion                   int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "isHoliday                     int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "onpromotion                   int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "isHoliday                     int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f133a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n",
      "Infinite values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Check for NaN\n",
    "print(\"NaN values per column:\\n\", submission.isna().sum())\n",
    "\n",
    "# Check for inf\n",
    "print(\"Infinite values per column:\\n\", np.isinf(submission).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb04ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, sales]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Find rows where sales is infinite\n",
    "mask = np.isinf(submission[\"sales\"])\n",
    "\n",
    "# Show the offending rows\n",
    "print(submission.loc[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b51f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Replace inf with NaN first (so ffill works)\n",
    "submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Forward fill\n",
    "submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# (Optional) if the very first row is NaN/inf, ffill won’t work — so backfill as fallback\n",
    "submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Save cleaned file\n",
    "submission.to_csv(\"my_submission_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42f2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Test set ID range: 3000888 to 3029399\n",
      "▶️ Submission ID range: 3000888 to 3029399\n",
      "✅ All submission IDs are inside test_df.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your files\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Check the range of IDs\n",
    "print(\"▶️ Test set ID range:\", test_df[\"id\"].min(), \"to\", test_df[\"id\"].max())\n",
    "print(\"▶️ Submission ID range:\", submission[\"id\"].min(), \"to\", submission[\"id\"].max())\n",
    "\n",
    "# Also check for IDs in submission but not in test\n",
    "extra_ids = set(submission[\"id\"]) - set(test_df[\"id\"])\n",
    "if extra_ids:\n",
    "    print(\"⚠️ IDs present in submission but not in test:\", list(extra_ids)[:10], \"...\")\n",
    "else:\n",
    "    print(\"✅ All submission IDs are inside test_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
