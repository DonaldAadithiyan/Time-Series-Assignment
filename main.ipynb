{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6329cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Checking sales data quality:\n",
      "Train sales - Min: 0.00, Max: 124717.00\n",
      "Negative sales count: 0\n",
      "Zero sales count: 939130\n",
      ">>> Creating time features...\n",
      ">>> Creating lag features per store-family...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_62526/3054959705.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarimax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSARIMAX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_62526/3054959705.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(group_df, target_col, lags)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Rolling means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{target_col}_rolling_mean_{window}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{target_col}_rolling_mean_{window}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Uni/Sem 5/Time Series-Assignment/venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4312\u001b[0m             \u001b[0;31m# Column to set is duplicated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4315\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Uni/Sem 5/Time Series-Assignment/venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTimeSeries\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconformed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrames\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4528\u001b[0m         \"\"\"\n\u001b[0;32m-> 4529\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4531\u001b[0m         if (\n\u001b[1;32m   4532\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Uni/Sem 5/Time Series-Assignment/venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5266\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# 0️⃣ Load & prepare data\n",
    "# ============================\n",
    "merged_train_df = pd.read_csv('merged_train_df.csv')\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "merged_train_df['date'] = pd.to_datetime(merged_train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "merged_train_df = merged_train_df.sort_values(\"date\").reset_index(drop=True)\n",
    "test_df = test_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "merged_train_df['store_nbr'] = merged_train_df['store_nbr'].astype(int)\n",
    "test_df['store_nbr'] = test_df['store_nbr'].astype(int)\n",
    "\n",
    "# ============================\n",
    "# CRITICAL FIX: Check for negative or zero sales\n",
    "# ============================\n",
    "print(\">>> Checking sales data quality:\")\n",
    "print(f\"Train sales - Min: {merged_train_df['sales'].min():.2f}, Max: {merged_train_df['sales'].max():.2f}\")\n",
    "print(f\"Negative sales count: {(merged_train_df['sales'] < 0).sum()}\")\n",
    "print(f\"Zero sales count: {(merged_train_df['sales'] == 0).sum()}\")\n",
    "\n",
    "# Remove or handle negative sales (critical for RMSLE)\n",
    "if (merged_train_df['sales'] < 0).any():\n",
    "    print(\">>> WARNING: Found negative sales values - replacing with 0\")\n",
    "    merged_train_df['sales'] = merged_train_df['sales'].clip(lower=0)\n",
    "\n",
    "# Add small constant to avoid log(0) in RMSLE calculation\n",
    "merged_train_df['sales'] = merged_train_df['sales'] + 0.001\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Simplified approach - use store-family level modeling\n",
    "# ============================\n",
    "def create_time_features(df):\n",
    "    \"\"\"Add time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(group_df, target_col='sales', lags=[1, 7, 14]):\n",
    "    \"\"\"Create lag features\"\"\"\n",
    "    df = group_df.copy().sort_values('date')\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling means\n",
    "    for window in [7, 14, 30]:\n",
    "        if len(df) > window:\n",
    "            df[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window, min_periods=1).mean()\n",
    "        else:\n",
    "            df[f'{target_col}_rolling_mean_{window}'] = df[target_col].mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\">>> Creating time features...\")\n",
    "train_fe = create_time_features(merged_train_df)\n",
    "test_fe = create_time_features(test_df)\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Create store-family combinations and lag features\n",
    "# ============================\n",
    "print(\">>> Creating lag features per store-family...\")\n",
    "\n",
    "train_with_lags = []\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    group_with_lags = create_lag_features(group)\n",
    "    train_with_lags.append(group_with_lags)\n",
    "\n",
    "train_fe = pd.concat(train_with_lags, ignore_index=True).sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# For test set, we need to be careful with lag features\n",
    "# Use last known values from training data\n",
    "print(\">>> Creating lag features for test set...\")\n",
    "\n",
    "test_with_lags = []\n",
    "for (store, family), group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    # Get corresponding training data for this store-family\n",
    "    train_group = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_group) == 0:\n",
    "        # No training data for this combination - use simple approach\n",
    "        group_with_lags = create_time_features(group.copy())\n",
    "        # Fill lag features with zeros or means\n",
    "        for lag in [1, 7, 14]:\n",
    "            group_with_lags[f'sales_lag_{lag}'] = 0\n",
    "        for window in [7, 14, 30]:\n",
    "            group_with_lags[f'sales_rolling_mean_{window}'] = 0\n",
    "        test_with_lags.append(group_with_lags)\n",
    "        continue\n",
    "    \n",
    "    # Use last values from training for initial lags\n",
    "    last_sales = train_group['sales'].tail(14).values  # Get last 14 values\n",
    "    \n",
    "    group_with_lags = group.copy()\n",
    "    \n",
    "    # Initialize lag features with last known values\n",
    "    for lag in [1, 7, 14]:\n",
    "        if lag <= len(last_sales):\n",
    "            initial_value = last_sales[-lag]\n",
    "        else:\n",
    "            initial_value = train_group['sales'].mean()\n",
    "        group_with_lags[f'sales_lag_{lag}'] = initial_value\n",
    "    \n",
    "    # Initialize rolling means\n",
    "    for window in [7, 14, 30]:\n",
    "        if len(last_sales) >= window:\n",
    "            initial_value = last_sales[-window:].mean()\n",
    "        else:\n",
    "            initial_value = train_group['sales'].mean()\n",
    "        group_with_lags[f'sales_rolling_mean_{window}'] = initial_value\n",
    "    \n",
    "    test_with_lags.append(group_with_lags)\n",
    "\n",
    "test_fe = pd.concat(test_with_lags, ignore_index=True).sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Feature engineering\n",
    "# ============================\n",
    "def feature_engineering_simple(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    df['promo_holiday'] = df['onpromotion'] * df['isHoliday'] if 'onpromotion' in df.columns and 'isHoliday' in df.columns else 0\n",
    "    \n",
    "    # Oil price features (if available)\n",
    "    if 'oil_price' in df.columns:\n",
    "        df['oil_price'] = df['oil_price'].fillna(df['oil_price'].mean())\n",
    "        df['oil_price_diff'] = df.groupby('store_nbr')['oil_price'].diff().fillna(0)\n",
    "    else:\n",
    "        df['oil_price'] = 0\n",
    "        df['oil_price_diff'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_fe = feature_engineering_simple(train_fe)\n",
    "test_fe = feature_engineering_simple(test_fe)\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Encode categoricals\n",
    "# ============================\n",
    "cat_cols = ['family', 'city', 'state', 'type'] if all(col in train_fe.columns for col in ['city', 'state', 'type']) else ['family']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train_fe.columns and col in test_fe.columns:\n",
    "        le = LabelEncoder()\n",
    "        combined_values = pd.concat([train_fe[col].astype(str), test_fe[col].astype(str)]).unique()\n",
    "        le.fit(combined_values)\n",
    "        train_fe[col + '_encoded'] = le.transform(train_fe[col].astype(str))\n",
    "        test_fe[col + '_encoded'] = le.transform(test_fe[col].astype(str))\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Simplified modeling approach\n",
    "# ============================\n",
    "feature_cols = ['onpromotion', 'year', 'month', 'day', 'dayofweek', 'quarter', \n",
    "                'is_weekend', 'is_month_start', 'is_month_end', 'promo_holiday',\n",
    "                'oil_price', 'oil_price_diff']\n",
    "\n",
    "# Add lag features\n",
    "lag_cols = [col for col in train_fe.columns if 'sales_lag_' in col or 'sales_rolling_mean_' in col]\n",
    "feature_cols.extend(lag_cols)\n",
    "\n",
    "# Add encoded categorical features\n",
    "encoded_cols = [col for col in train_fe.columns if col.endswith('_encoded')]\n",
    "feature_cols.extend(encoded_cols)\n",
    "\n",
    "# Keep only available features\n",
    "feature_cols = [col for col in feature_cols if col in train_fe.columns and col in test_fe.columns]\n",
    "\n",
    "print(f\">>> Using {len(feature_cols)} features: {feature_cols}\")\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Train models per store-family combination\n",
    "# ============================\n",
    "predictions = []\n",
    "\n",
    "store_family_combinations = set(\n",
    "    zip(train_fe['store_nbr'], train_fe['family'])\n",
    ").intersection(set(zip(test_fe['store_nbr'], test_fe['family'])))\n",
    "\n",
    "print(f\">>> Training models for {len(store_family_combinations)} store-family combinations...\")\n",
    "\n",
    "for i, (store_num, family) in enumerate(sorted(store_family_combinations)):\n",
    "    if i % 100 == 0:\n",
    "        print(f\">>> Processing combination {i+1}/{len(store_family_combinations)}\")\n",
    "    \n",
    "    # Get data for this store-family combination\n",
    "    train_mask = (train_fe['store_nbr'] == store_num) & (train_fe['family'] == family)\n",
    "    test_mask = (test_fe['store_nbr'] == store_num) & (test_fe['family'] == family)\n",
    "    \n",
    "    train_subset = train_fe[train_mask].sort_values('date')\n",
    "    test_subset = test_fe[test_mask].sort_values('date')\n",
    "    \n",
    "    if len(train_subset) < 5 or len(test_subset) == 0:\n",
    "        continue\n",
    "    \n",
    "    y_train = train_subset['sales']\n",
    "    \n",
    "    # Prepare features\n",
    "    X_train = train_subset[feature_cols].fillna(0)\n",
    "    X_test = test_subset[feature_cols].fillna(0)\n",
    "    \n",
    "    # Simple approach: use median/mean as baseline with trend adjustment\n",
    "    try:\n",
    "        # Calculate recent trend\n",
    "        recent_sales = y_train.tail(min(14, len(y_train)))\n",
    "        median_sales = recent_sales.median()\n",
    "        \n",
    "        # Simple trend calculation\n",
    "        if len(recent_sales) > 7:\n",
    "            early_avg = recent_sales.head(7).mean()\n",
    "            late_avg = recent_sales.tail(7).mean()\n",
    "            trend_factor = late_avg / early_avg if early_avg > 0 else 1.0\n",
    "            trend_factor = np.clip(trend_factor, 0.5, 2.0)  # Limit extreme trends\n",
    "        else:\n",
    "            trend_factor = 1.0\n",
    "        \n",
    "        # Adjust for promotions if available\n",
    "        if 'onpromotion' in test_subset.columns:\n",
    "            promo_boost = test_subset['onpromotion'].mean() * 0.1 + 1.0  # 10% boost for promotions\n",
    "        else:\n",
    "            promo_boost = 1.0\n",
    "        \n",
    "        # Generate predictions\n",
    "        base_prediction = median_sales * trend_factor * promo_boost\n",
    "        \n",
    "        # Add some seasonality based on day of week\n",
    "        if 'dayofweek' in test_subset.columns:\n",
    "            dow_factors = []\n",
    "            for dow in test_subset['dayofweek']:\n",
    "                train_dow = train_subset[train_subset['dayofweek'] == dow]['sales']\n",
    "                if len(train_dow) > 0:\n",
    "                    dow_factor = train_dow.mean() / y_train.mean() if y_train.mean() > 0 else 1.0\n",
    "                    dow_factors.append(dow_factor)\n",
    "                else:\n",
    "                    dow_factors.append(1.0)\n",
    "            \n",
    "            predictions_array = base_prediction * np.array(dow_factors)\n",
    "        else:\n",
    "            predictions_array = np.full(len(test_subset), base_prediction)\n",
    "        \n",
    "        # Ensure predictions are positive\n",
    "        predictions_array = np.maximum(predictions_array, 0.001)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback: use simple median\n",
    "        predictions_array = np.full(len(test_subset), max(y_train.median(), 0.001))\n",
    "    \n",
    "    # Create output\n",
    "    output_df = test_subset[['id']].copy()\n",
    "    output_df['sales'] = predictions_array\n",
    "    predictions.append(output_df)\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Create final submission\n",
    "# ============================\n",
    "if predictions:\n",
    "    final_preds = pd.concat(predictions, ignore_index=True)\n",
    "    \n",
    "    # Ensure all test IDs are covered\n",
    "    missing_ids = set(test_fe['id']) - set(final_preds['id'])\n",
    "    if missing_ids:\n",
    "        print(f\">>> Found {len(missing_ids)} missing predictions, filling with median...\")\n",
    "        overall_median = train_fe['sales'].median()\n",
    "        \n",
    "        missing_df = pd.DataFrame({\n",
    "            'id': list(missing_ids),\n",
    "            'sales': [overall_median] * len(missing_ids)\n",
    "        })\n",
    "        final_preds = pd.concat([final_preds, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Final cleanup\n",
    "    submission_df = final_preds.copy()\n",
    "    submission_df['sales'] = submission_df['sales'].fillna(1.0)  # Fill any remaining NaN\n",
    "    submission_df['sales'] = np.maximum(submission_df['sales'], 0.001)  # Ensure positive\n",
    "    \n",
    "    # Sort by ID for submission\n",
    "    submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    print(f\">>> Final submission shape: {submission_df.shape}\")\n",
    "    print(f\">>> Sales statistics - Min: {submission_df['sales'].min():.3f}, Max: {submission_df['sales'].max():.3f}, Median: {submission_df['sales'].median():.3f}\")\n",
    "    \n",
    "    submission_df.to_csv(\"improved_submission.csv\", index=False)\n",
    "    print(\">>> Improved submission saved to 'improved_submission.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\">>> ERROR: No predictions generated!\")\n",
    "\n",
    "# ============================\n",
    "# 8️⃣ Additional debugging\n",
    "# ============================\n",
    "print(\"\\n>>> DEBUGGING INFO:\")\n",
    "print(f\">>> Total store-family combinations in train: {train_fe.groupby(['store_nbr', 'family']).ngroups}\")\n",
    "print(f\">>> Total store-family combinations in test: {test_fe.groupby(['store_nbr', 'family']).ngroups}\")\n",
    "print(f\">>> Common combinations: {len(store_family_combinations)}\")\n",
    "\n",
    "if predictions:\n",
    "    print(f\">>> Predictions generated for: {len(predictions)} combinations\")\n",
    "    print(f\">>> Total prediction rows: {len(final_preds)}\")\n",
    "    print(f\">>> Expected test rows: {len(test_fe)}\")\n",
    "\n",
    "print(\">>> Feature columns used:\", feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d245dcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "sales                       float64\n",
       "onpromotion                   int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "isHoliday                     int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "onpromotion                   int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "isHoliday                     int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f133a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n",
      "Infinite values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Check for NaN\n",
    "print(\"NaN values per column:\\n\", submission.isna().sum())\n",
    "\n",
    "# Check for inf\n",
    "print(\"Infinite values per column:\\n\", np.isinf(submission).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb04ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, sales]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Find rows where sales is infinite\n",
    "mask = np.isinf(submission[\"sales\"])\n",
    "\n",
    "# Show the offending rows\n",
    "print(submission.loc[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b51f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Replace inf with NaN first (so ffill works)\n",
    "submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Forward fill\n",
    "submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# (Optional) if the very first row is NaN/inf, ffill won’t work — so backfill as fallback\n",
    "submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Save cleaned file\n",
    "submission.to_csv(\"my_submission_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42f2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Test set ID range: 3000888 to 3029399\n",
      "▶️ Submission ID range: 3000888 to 3029399\n",
      "✅ All submission IDs are inside test_df.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your files\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Check the range of IDs\n",
    "print(\"▶️ Test set ID range:\", test_df[\"id\"].min(), \"to\", test_df[\"id\"].max())\n",
    "print(\"▶️ Submission ID range:\", submission[\"id\"].min(), \"to\", submission[\"id\"].max())\n",
    "\n",
    "# Also check for IDs in submission but not in test\n",
    "extra_ids = set(submission[\"id\"]) - set(test_df[\"id\"])\n",
    "if extra_ids:\n",
    "    print(\"⚠️ IDs present in submission but not in test:\", list(extra_ids)[:10], \"...\")\n",
    "else:\n",
    "    print(\"✅ All submission IDs are inside test_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
