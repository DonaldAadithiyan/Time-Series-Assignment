{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6329cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Enhanced data preprocessing with validation...\n",
      "Train sales - Min: 0.00, Max: 124717.00\n",
      "Negative sales: 0, Zero sales: 939130\n",
      ">>> Train period: 2013-01-01 00:00:00 to 2017-07-31 00:00:00\n",
      ">>> Validation period: 2017-08-01 00:00:00 to 2017-08-15 00:00:00\n",
      ">>> Test period: 2017-08-16 00:00:00 to 2017-08-31 00:00:00\n",
      ">>> Creating advanced features...\n",
      ">>> Creating enhanced lag features...\n",
      ">>> Creating lag features for validation...\n",
      ">>> Creating lag features for test...\n",
      ">>> Creating aggregate features...\n",
      ">>> Using 70 features\n",
      ">>> Train samples: 2974158\n",
      ">>> Validation samples: 26730\n",
      ">>> Test samples: 28512\n",
      ">>> Training Random Forest...\n",
      ">>> Random Forest validation RMSLE: 0.5066\n",
      ">>> Training XGBoost...\n",
      ">>> XGBoost validation RMSLE: 0.6172\n",
      ">>> Training Neural Network...\n",
      ">>> Neural Network validation RMSLE: 0.6511\n",
      ">>> Enhanced ARIMA component...\n",
      ">>> Selecting combinations for enhanced time series...\n",
      ">>> Processing time series for 300 combinations...\n",
      ">>> Time series progress: 1/300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# 0Ô∏è‚É£ Load & prepare data (KEEPING YOUR EXACT WORKING VERSION)\n",
    "# ============================\n",
    "merged_train_df = pd.read_csv('merged_train_df.csv')\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "merged_train_df['date'] = pd.to_datetime(merged_train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "merged_train_df = merged_train_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values([\"store_nbr\", \"family\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "merged_train_df['store_nbr'] = merged_train_df['store_nbr'].astype(int)\n",
    "test_df['store_nbr'] = test_df['store_nbr'].astype(int)\n",
    "\n",
    "# ============================\n",
    "# 1Ô∏è‚É£ Advanced data preprocessing with LOG TRANSFORM\n",
    "# ============================\n",
    "print(\">>> Advanced data preprocessing with log transform...\")\n",
    "\n",
    "# Handle sales data quality\n",
    "print(f\"Train sales - Min: {merged_train_df['sales'].min():.2f}, Max: {merged_train_df['sales'].max():.2f}\")\n",
    "print(f\"Negative sales: {(merged_train_df['sales'] < 0).sum()}, Zero sales: {(merged_train_df['sales'] == 0).sum()}\")\n",
    "\n",
    "# Clip negative values and add small epsilon for log\n",
    "merged_train_df['sales'] = np.maximum(merged_train_df['sales'], 0.01)\n",
    "merged_train_df['log_sales'] = np.log1p(merged_train_df['sales'])  # This will be our target\n",
    "\n",
    "# Get date ranges\n",
    "train_start = merged_train_df['date'].min()\n",
    "train_end = merged_train_df['date'].max()\n",
    "test_start = test_df['date'].min()\n",
    "test_end = test_df['date'].max()\n",
    "\n",
    "print(f\">>> Train period: {train_start} to {train_end}\")\n",
    "print(f\">>> Test period: {test_start} to {test_end}\")\n",
    "print(f\">>> Log sales range: {merged_train_df['log_sales'].min():.3f} to {merged_train_df['log_sales'].max():.3f}\")\n",
    "\n",
    "# ============================\n",
    "# 2Ô∏è‚É£ Simplified feature engineering (KEEPING YOUR EXACT VERSION)\n",
    "# ============================\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['week'] = df['date'].dt.isocalendar().week\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Advanced time features\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_monday'] = (df['dayofweek'] == 0).astype(int)\n",
    "    df['is_friday'] = (df['dayofweek'] == 4).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_other_features(df):\n",
    "    \"\"\"Add non-time features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Store-family identifier\n",
    "    df['store_family'] = df['store_nbr'].astype(str) + \"_\" + df['family'].astype(str)\n",
    "    \n",
    "    # Promotion features\n",
    "    if 'onpromotion' in df.columns:\n",
    "        df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['onpromotion'] = 0\n",
    "    \n",
    "    # Holiday features\n",
    "    if 'isHoliday' in df.columns:\n",
    "        df['isHoliday'] = df['isHoliday'].fillna(0).astype(int)\n",
    "        df['promo_holiday'] = df['onpromotion'] * df['isHoliday']\n",
    "    else:\n",
    "        df['isHoliday'] = 0\n",
    "        df['promo_holiday'] = 0\n",
    "    \n",
    "    # Oil price features\n",
    "    if 'oil_price' in df.columns:\n",
    "        df['oil_price'] = df['oil_price'].fillna(method='ffill').fillna(method='bfill').fillna(100)\n",
    "    else:\n",
    "        df['oil_price'] = 100\n",
    "    \n",
    "    # Special event features\n",
    "    for col in ['earthquake_impact', 'salary_day_impact']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features_safe(df_list):\n",
    "    \"\"\"Create lag features safely for list of dataframes (one per store-family) - WITH LOG TRANSFORM\"\"\"\n",
    "    \n",
    "    result_dfs = []\n",
    "    \n",
    "    for df_group in df_list:\n",
    "        if len(df_group) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_group = df_group.copy().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Use LOG SALES for lag features\n",
    "        if 'log_sales' in df_group.columns:\n",
    "            target_col = 'log_sales'\n",
    "        else:\n",
    "            target_col = 'sales'  # fallback for test data\n",
    "        \n",
    "        # Simple lag features\n",
    "        df_group['sales_lag_1'] = df_group[target_col].shift(1).fillna(df_group[target_col].mean())\n",
    "        df_group['sales_lag_7'] = df_group[target_col].shift(7).fillna(df_group[target_col].mean())\n",
    "        df_group['sales_lag_14'] = df_group[target_col].shift(14).fillna(df_group[target_col].mean())\n",
    "        \n",
    "        # Simple rolling features - avoid groupby issues\n",
    "        df_group['sales_roll_7_mean'] = df_group[target_col].rolling(7, min_periods=1).mean()\n",
    "        df_group['sales_roll_14_mean'] = df_group[target_col].rolling(14, min_periods=1).mean()\n",
    "        df_group['sales_roll_7_std'] = df_group[target_col].rolling(7, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Simple trend feature\n",
    "        if len(df_group) >= 7:\n",
    "            df_group['sales_trend_7'] = (\n",
    "                df_group[target_col].rolling(7, min_periods=1).mean() / \n",
    "                df_group[target_col].rolling(14, min_periods=1).mean().shift(7).fillna(df_group[target_col].mean())\n",
    "            ).fillna(1.0)\n",
    "        else:\n",
    "            df_group['sales_trend_7'] = 1.0\n",
    "        \n",
    "        result_dfs.append(df_group)\n",
    "    \n",
    "    return result_dfs\n",
    "\n",
    "# Apply basic feature engineering\n",
    "print(\">>> Creating time features...\")\n",
    "train_fe = create_time_features(merged_train_df)\n",
    "test_fe = create_time_features(test_df)\n",
    "\n",
    "print(\">>> Adding other features...\")\n",
    "train_fe = add_other_features(train_fe)\n",
    "test_fe = add_other_features(test_fe)\n",
    "\n",
    "# Create lag features by processing each store-family group separately\n",
    "print(\">>> Creating lag features with log transform...\")\n",
    "\n",
    "# Split training data by store-family\n",
    "train_groups = []\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    train_groups.append(group)\n",
    "\n",
    "# Process lag features\n",
    "train_groups_with_lags = create_lag_features_safe(train_groups)\n",
    "\n",
    "# Recombine training data\n",
    "if train_groups_with_lags:\n",
    "    train_fe = pd.concat(train_groups_with_lags, ignore_index=True)\n",
    "    train_fe = train_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# For test data, create lag features using training data statistics (in log space)\n",
    "print(\">>> Creating lag features for test...\")\n",
    "\n",
    "test_with_lags = []\n",
    "for (store, family), test_group in test_fe.groupby(['store_nbr', 'family']):\n",
    "    test_group = test_group.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Find corresponding training group\n",
    "    train_group = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    \n",
    "    if len(train_group) > 0:\n",
    "        # Use last values from training for lag initialization (in log space)\n",
    "        recent_log_sales = train_group['log_sales'].tail(14)\n",
    "        \n",
    "        # Initialize lag features\n",
    "        if len(recent_log_sales) >= 1:\n",
    "            test_group['sales_lag_1'] = recent_log_sales.iloc[-1]\n",
    "        else:\n",
    "            test_group['sales_lag_1'] = train_group['log_sales'].mean()\n",
    "            \n",
    "        if len(recent_log_sales) >= 7:\n",
    "            test_group['sales_lag_7'] = recent_log_sales.iloc[-7]\n",
    "        else:\n",
    "            test_group['sales_lag_7'] = train_group['log_sales'].mean()\n",
    "            \n",
    "        if len(recent_log_sales) >= 14:\n",
    "            test_group['sales_lag_14'] = recent_log_sales.iloc[-14]\n",
    "        else:\n",
    "            test_group['sales_lag_14'] = train_group['log_sales'].mean()\n",
    "        \n",
    "        # Rolling features - use recent statistics\n",
    "        test_group['sales_roll_7_mean'] = recent_log_sales.tail(7).mean() if len(recent_log_sales) >= 7 else train_group['log_sales'].mean()\n",
    "        test_group['sales_roll_14_mean'] = recent_log_sales.mean()\n",
    "        test_group['sales_roll_7_std'] = recent_log_sales.tail(7).std() if len(recent_log_sales) >= 7 else train_group['log_sales'].std()\n",
    "        \n",
    "        if pd.isna(test_group['sales_roll_7_std'].iloc[0]):\n",
    "            test_group['sales_roll_7_std'] = 0\n",
    "            \n",
    "        # Trend feature\n",
    "        test_group['sales_trend_7'] = 1.0\n",
    "        \n",
    "    else:\n",
    "        # No training data - use defaults (use log space defaults)\n",
    "        default_log_sales = np.log1p(1.0)\n",
    "        for col in ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_roll_7_mean', 'sales_roll_14_mean']:\n",
    "            test_group[col] = default_log_sales\n",
    "        test_group['sales_roll_7_std'] = 0.0\n",
    "        test_group['sales_trend_7'] = 1.0\n",
    "    \n",
    "    test_with_lags.append(test_group)\n",
    "\n",
    "if test_with_lags:\n",
    "    test_fe = pd.concat(test_with_lags, ignore_index=True)\n",
    "    test_fe = test_fe.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# 3Ô∏è‚É£ Add aggregate features (KEEPING YOUR EXACT VERSION - but with log sales)\n",
    "# ============================\n",
    "print(\">>> Creating aggregate features (log space)...\")\n",
    "\n",
    "# Store level statistics from training data (using log_sales)\n",
    "store_stats = train_fe.groupby('store_nbr')['log_sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "store_stats.columns = ['store_nbr', 'store_sales_mean', 'store_sales_std', 'store_sales_median']\n",
    "store_stats['store_sales_std'] = store_stats['store_sales_std'].fillna(0)\n",
    "\n",
    "# Family level statistics\n",
    "family_stats = train_fe.groupby('family')['log_sales'].agg(['mean', 'std', 'median']).reset_index()\n",
    "family_stats.columns = ['family', 'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "family_stats['family_sales_std'] = family_stats['family_sales_std'].fillna(0)\n",
    "\n",
    "# Add to training data\n",
    "train_fe = train_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "train_fe = train_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Add to test data\n",
    "test_fe = test_fe.merge(store_stats, on='store_nbr', how='left')\n",
    "test_fe = test_fe.merge(family_stats, on='family', how='left')\n",
    "\n",
    "# Fill missing aggregate features\n",
    "agg_cols = ['store_sales_mean', 'store_sales_std', 'store_sales_median', \n",
    "            'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "\n",
    "for col in agg_cols:\n",
    "    overall_median = train_fe[col].median()\n",
    "    train_fe[col] = train_fe[col].fillna(overall_median)\n",
    "    test_fe[col] = test_fe[col].fillna(overall_median)\n",
    "\n",
    "# ============================\n",
    "# 4Ô∏è‚É£ Encode categoricals (KEEPING YOUR EXACT VERSION)\n",
    "# ============================\n",
    "cat_cols = ['family']\n",
    "if all(col in train_fe.columns and col in test_fe.columns for col in ['city', 'state', 'type']):\n",
    "    cat_cols.extend(['city', 'state', 'type'])\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined_values = pd.concat([train_fe[col].astype(str), test_fe[col].astype(str)]).unique()\n",
    "    le.fit(combined_values)\n",
    "    train_fe[f'{col}_encoded'] = le.transform(train_fe[col].astype(str))\n",
    "    test_fe[f'{col}_encoded'] = le.transform(test_fe[col].astype(str))\n",
    "\n",
    "# ============================\n",
    "# 5Ô∏è‚É£ Feature selection (KEEPING YOUR EXACT VERSION)\n",
    "# ============================\n",
    "base_features = [\n",
    "    'year', 'month', 'day', 'dayofweek', 'quarter', 'week', 'dayofyear',\n",
    "    'is_weekend', 'is_monday', 'is_friday', 'is_month_start', 'is_month_end',\n",
    "    'is_quarter_start', 'is_quarter_end',\n",
    "    'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'day_sin', 'day_cos',\n",
    "    'onpromotion', 'isHoliday', 'promo_holiday', 'oil_price', 'earthquake_impact', 'salary_day_impact'\n",
    "]\n",
    "\n",
    "lag_features = ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', \n",
    "                'sales_roll_7_mean', 'sales_roll_14_mean', 'sales_roll_7_std', 'sales_trend_7']\n",
    "\n",
    "aggregate_features = ['store_sales_mean', 'store_sales_std', 'store_sales_median',\n",
    "                     'family_sales_mean', 'family_sales_std', 'family_sales_median']\n",
    "\n",
    "encoded_features = [f'{col}_encoded' for col in cat_cols]\n",
    "\n",
    "all_features = base_features + lag_features + aggregate_features + encoded_features\n",
    "available_features = [col for col in all_features if col in train_fe.columns and col in test_fe.columns]\n",
    "\n",
    "print(f\">>> Using {len(available_features)} features\")\n",
    "\n",
    "# ============================\n",
    "# 6Ô∏è‚É£ Random Forest training with REDUCED OVERFITTING + LOG TARGET\n",
    "# ============================\n",
    "print(\">>> Training Random Forest model (log space, reduced overfitting)...\")\n",
    "\n",
    "# Clean training data\n",
    "train_clean = train_fe.dropna(subset=lag_features[:3])  # Only require basic lags\n",
    "print(f\">>> Clean training samples: {len(train_clean)}\")\n",
    "\n",
    "if len(train_clean) > 500:\n",
    "    # Prepare data\n",
    "    X_train = train_clean[available_features].fillna(0)\n",
    "    y_train = train_clean['log_sales']  # TARGET IS NOW LOG_SALES!\n",
    "    X_test = test_fe[available_features].fillna(0)\n",
    "    \n",
    "    # Train Random Forest - REDUCED OVERFITTING PARAMETERS\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=150,      # Increased from 100 for better generalization\n",
    "        max_depth=10,          # Reduced from 12 to prevent overfitting\n",
    "        min_samples_split=20,  # Increased from 10 to prevent overfitting\n",
    "        min_samples_leaf=10,   # Increased from 5 to prevent overfitting\n",
    "        max_features='sqrt',   # Added to reduce overfitting\n",
    "        bootstrap=True,        # Ensure bootstrap sampling\n",
    "        max_samples=0.8,       # Use 80% of data per tree\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (in log space)\n",
    "    rf_log_predictions = rf_model.predict(X_test)\n",
    "    \n",
    "    # Convert back to original space\n",
    "    rf_predictions = np.expm1(rf_log_predictions)  # Inverse of log1p\n",
    "    rf_predictions = np.maximum(rf_predictions, 0.01)\n",
    "    \n",
    "    print(\">>> Random Forest training completed (with log transform)\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\">>> Top 10 RF features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    rf_predictions = None\n",
    "    rf_log_predictions = None\n",
    "    print(\">>> Insufficient training data for Random Forest\")\n",
    "\n",
    "# ============================\n",
    "# 7Ô∏è‚É£ ARIMA Model - ENHANCED WITH LOG TRANSFORM\n",
    "# ============================\n",
    "print(\">>> Adding ARIMA component (enhanced, log space)...\")\n",
    "\n",
    "def fit_enhanced_arima(train_data, forecast_periods, max_data_points=120):\n",
    "    \"\"\"Enhanced ARIMA model for time series patterns (log space)\"\"\"\n",
    "    \n",
    "    if len(train_data) < 28:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use recent data and work in log space\n",
    "        recent_data = train_data.sort_values('date').tail(max_data_points)\n",
    "        log_sales_series = recent_data['log_sales'].values\n",
    "        \n",
    "        # Simple parameter selection with more configurations\n",
    "        best_model = None\n",
    "        best_aic = float('inf')\n",
    "        \n",
    "        # Try more ARIMA configurations\n",
    "        configs = [\n",
    "            (1,1,1), (2,1,1), (1,1,2), (0,1,1), (1,1,0), (2,1,2),\n",
    "            (3,1,1), (1,1,3), (2,1,0), (0,1,2), (1,0,1), (2,0,2)  # Additional configs\n",
    "        ]\n",
    "        \n",
    "        for p, d, q in configs:\n",
    "            try:\n",
    "                model = ARIMA(log_sales_series, order=(p, d, q))\n",
    "                fitted_model = model.fit()\n",
    "                \n",
    "                if fitted_model.aic < best_aic:\n",
    "                    best_aic = fitted_model.aic\n",
    "                    best_model = fitted_model\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_model is not None:\n",
    "            log_forecast = best_model.forecast(steps=forecast_periods)\n",
    "            # Convert back to original space\n",
    "            forecast = np.expm1(log_forecast)\n",
    "            forecast = np.maximum(forecast, 0.01)\n",
    "            return forecast\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Select MORE store-family combinations for ARIMA (increased impact)\n",
    "print(\">>> Selecting combinations for ARIMA (increased coverage)...\")\n",
    "arima_combinations = []\n",
    "\n",
    "for (store, family), group in train_fe.groupby(['store_nbr', 'family']):\n",
    "    if len(group) >= 42:  # Reduced threshold from 50 to get more combinations\n",
    "        volume = group['sales'].sum()\n",
    "        data_points = len(group)\n",
    "        score = volume * np.log(data_points)  # Simple scoring\n",
    "        arima_combinations.append((store, family, score))\n",
    "\n",
    "# Sort and take top 200 combinations (increased from 100)\n",
    "arima_combinations.sort(key=lambda x: x[2], reverse=True)\n",
    "top_arima_combinations = [x[:2] for x in arima_combinations[:200]]\n",
    "\n",
    "print(f\">>> Processing ARIMA for {len(top_arima_combinations)} combinations...\")\n",
    "\n",
    "arima_results = []\n",
    "for i, (store, family) in enumerate(top_arima_combinations):\n",
    "    if i % 40 == 0:\n",
    "        print(f\">>> ARIMA progress: {i+1}/{len(top_arima_combinations)}\")\n",
    "        \n",
    "    train_subset = train_fe[(train_fe['store_nbr'] == store) & (train_fe['family'] == family)]\n",
    "    test_subset = test_fe[(test_fe['store_nbr'] == store) & (test_fe['family'] == family)]\n",
    "    \n",
    "    if len(test_subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    arima_pred = fit_enhanced_arima(train_subset, len(test_subset))\n",
    "    \n",
    "    if arima_pred is not None:\n",
    "        result_df = test_subset[['id']].copy()\n",
    "        result_df['sales_arima'] = arima_pred\n",
    "        arima_results.append(result_df)\n",
    "\n",
    "# Combine ARIMA results\n",
    "if arima_results:\n",
    "    arima_df = pd.concat(arima_results, ignore_index=True)\n",
    "    print(f\">>> ARIMA predictions generated for {len(arima_df)} test samples\")\n",
    "else:\n",
    "    arima_df = pd.DataFrame(columns=['id', 'sales_arima'])\n",
    "\n",
    "# ============================\n",
    "# 8Ô∏è‚É£ Store-family level predictions (KEEPING YOUR EXACT VERSION but with log awareness)\n",
    "# ============================\n",
    "print(\">>> Creating store-family level predictions...\")\n",
    "\n",
    "store_family_preds = []\n",
    "store_family_combinations = set(\n",
    "    zip(train_fe['store_nbr'], train_fe['family'])\n",
    ").intersection(set(zip(test_fe['store_nbr'], test_fe['family'])))\n",
    "\n",
    "for i, (store_num, family) in enumerate(sorted(store_family_combinations)):\n",
    "    if i % 300 == 0:\n",
    "        print(f\">>> Processing {i+1}/{len(store_family_combinations)}\")\n",
    "    \n",
    "    train_subset = train_fe[\n",
    "        (train_fe['store_nbr'] == store_num) & (train_fe['family'] == family)\n",
    "    ].sort_values('date').tail(100)  # Use recent data\n",
    "    \n",
    "    test_subset = test_fe[\n",
    "        (test_fe['store_nbr'] == store_num) & (test_fe['family'] == family)\n",
    "    ].sort_values('date')\n",
    "    \n",
    "    if len(train_subset) < 10 or len(test_subset) == 0:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        recent_sales = train_subset['sales'].values\n",
    "        \n",
    "        # Advanced prediction combining multiple signals\n",
    "        base_pred = np.median(recent_sales[-21:]) if len(recent_sales) >= 21 else np.median(recent_sales)\n",
    "        \n",
    "        # Trend (last month vs previous month)\n",
    "        if len(recent_sales) >= 28:\n",
    "            recent_avg = np.mean(recent_sales[-14:])\n",
    "            prev_avg = np.mean(recent_sales[-28:-14])\n",
    "            trend_factor = recent_avg / (prev_avg + 0.01)\n",
    "            trend_factor = np.clip(trend_factor, 0.8, 1.3)\n",
    "        else:\n",
    "            trend_factor = 1.0\n",
    "        \n",
    "        # Seasonality (day of week)\n",
    "        seasonal_factors = []\n",
    "        train_df_subset = train_subset.copy()\n",
    "        \n",
    "        for _, row in test_subset.iterrows():\n",
    "            dow = row['dayofweek']\n",
    "            dow_sales = train_df_subset[train_df_subset['dayofweek'] == dow]['sales']\n",
    "            \n",
    "            if len(dow_sales) >= 3:\n",
    "                dow_factor = dow_sales.mean() / (train_df_subset['sales'].mean() + 0.01)\n",
    "                seasonal_factors.append(np.clip(dow_factor, 0.7, 1.5))\n",
    "            else:\n",
    "                seasonal_factors.append(1.0)\n",
    "        \n",
    "        # Promotion effect\n",
    "        promo_factors = []\n",
    "        if 'onpromotion' in test_subset.columns:\n",
    "            # Calculate promotion lift from training data\n",
    "            promo_sales = train_df_subset[train_df_subset['onpromotion'] == 1]['sales']\n",
    "            normal_sales = train_df_subset[train_df_subset['onpromotion'] == 0]['sales']\n",
    "            \n",
    "            if len(promo_sales) > 0 and len(normal_sales) > 0:\n",
    "                promo_lift = promo_sales.mean() / (normal_sales.mean() + 0.01)\n",
    "                promo_lift = np.clip(promo_lift, 1.0, 1.8)\n",
    "            else:\n",
    "                promo_lift = 1.15  # Default 15% lift\n",
    "            \n",
    "            for _, row in test_subset.iterrows():\n",
    "                if row['onpromotion'] == 1:\n",
    "                    promo_factors.append(promo_lift)\n",
    "                else:\n",
    "                    promo_factors.append(1.0)\n",
    "        else:\n",
    "            promo_factors = [1.0] * len(test_subset)\n",
    "        \n",
    "        # Combine all factors\n",
    "        predictions_array = (\n",
    "            base_pred * trend_factor * \n",
    "            np.array(seasonal_factors) * \n",
    "            np.array(promo_factors)\n",
    "        )\n",
    "        \n",
    "        predictions_array = np.maximum(predictions_array, 0.01)\n",
    "        \n",
    "    except:\n",
    "        predictions_array = np.full(len(test_subset), max(np.median(recent_sales), 0.01))\n",
    "    \n",
    "    # Store results\n",
    "    output_df = test_subset[['id']].copy()\n",
    "    output_df['sales'] = predictions_array\n",
    "    store_family_preds.append(output_df)\n",
    "\n",
    "# ============================\n",
    "# 9Ô∏è‚É£ Final RF+ARIMA ensemble - INCREASED ARIMA IMPACT\n",
    "# ============================\n",
    "print(\">>> Creating RF+ARIMA ensemble (increased ARIMA impact)...\")\n",
    "\n",
    "if store_family_preds:\n",
    "    sf_pred_df = pd.concat(store_family_preds, ignore_index=True)\n",
    "    \n",
    "    if rf_predictions is not None:\n",
    "        # Create RF prediction dataframe\n",
    "        rf_pred_df = test_fe[['id']].copy()\n",
    "        rf_pred_df['sales'] = rf_predictions\n",
    "        \n",
    "        # Merge all predictions\n",
    "        ensemble_df = sf_pred_df.merge(rf_pred_df, on='id', how='outer', suffixes=('_sf', '_rf'))\n",
    "        \n",
    "        # Add ARIMA if available\n",
    "        if len(arima_df) > 0:\n",
    "            ensemble_df = ensemble_df.merge(arima_df[['id', 'sales_arima']], on='id', how='left')\n",
    "        else:\n",
    "            ensemble_df['sales_arima'] = np.nan\n",
    "        \n",
    "        # Fill missing values\n",
    "        ensemble_df['sales_sf'] = ensemble_df['sales_sf'].fillna(ensemble_df['sales_rf'])\n",
    "        ensemble_df['sales_rf'] = ensemble_df['sales_rf'].fillna(ensemble_df['sales_sf'])\n",
    "        \n",
    "        # RF+SF ensemble - slightly more conservative\n",
    "        ensemble_df['sales'] = ensemble_df['sales_rf'] * 0.55 + ensemble_df['sales_sf'] * 0.45  # More balanced\n",
    "        \n",
    "        # Add ARIMA component where available - INCREASED IMPACT\n",
    "        arima_mask = ~pd.isna(ensemble_df['sales_arima'])\n",
    "        if arima_mask.sum() > 0:\n",
    "            # Increased ARIMA weight: 50% base ensemble + 50% ARIMA (was 70%/30%)\n",
    "            ensemble_df.loc[arima_mask, 'sales'] = (\n",
    "                ensemble_df.loc[arima_mask, 'sales'] * 0.5 + \n",
    "                ensemble_df.loc[arima_mask, 'sales_arima'] * 0.5\n",
    "            )\n",
    "        \n",
    "        final_submission = ensemble_df[['id', 'sales']].copy()\n",
    "        \n",
    "    else:\n",
    "        final_submission = sf_pred_df.copy()\n",
    "    \n",
    "    # Handle missing predictions\n",
    "    all_test_ids = set(test_fe['id'])\n",
    "    predicted_ids = set(final_submission['id'])\n",
    "    missing_ids = all_test_ids - predicted_ids\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\">>> Filling {len(missing_ids)} missing predictions...\")\n",
    "        median_sales = train_fe['sales'].median()\n",
    "        missing_df = pd.DataFrame({'id': list(missing_ids), 'sales': [median_sales] * len(missing_ids)})\n",
    "        final_submission = pd.concat([final_submission, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Final cleanup\n",
    "    final_submission['sales'] = final_submission['sales'].fillna(train_fe['sales'].median())\n",
    "    final_submission['sales'] = np.maximum(final_submission['sales'], 0.01)\n",
    "    final_submission = final_submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    # Save\n",
    "    final_submission.to_csv(\"enhanced_rf_arima_log_submission.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n>>> ENHANCED RF+ARIMA ENSEMBLE RESULTS (LOG TRANSFORM):\")\n",
    "    print(f\">>> Submission shape: {final_submission.shape}\")\n",
    "    print(f\">>> Sales range: {final_submission['sales'].min():.3f} - {final_submission['sales'].max():.3f}\")\n",
    "    print(f\">>> Sales median: {final_submission['sales'].median():.3f}\")\n",
    "    print(f\">>> Random Forest coverage: 100% (trained on log_sales)\")\n",
    "    print(f\">>> ARIMA coverage: {len(arima_df) / len(final_submission) * 100:.1f}%\")\n",
    "    print(f\">>> Store-family coverage: {len(sf_pred_df) / len(final_submission) * 100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n>>> ENHANCED ENSEMBLE COMPOSITION:\")\n",
    "    arima_coverage = len(arima_df) / len(final_submission) if len(final_submission) > 0 else 0\n",
    "    print(f\">>> For {arima_coverage*100:.1f}% of predictions: 50% (RF+SF) + 50% ARIMA (INCREASED)\")\n",
    "    print(f\">>> For remaining predictions: 55% RF + 45% Store-Family\")\n",
    "    print(f\">>> Key improvements:\")\n",
    "    print(f\"    - Random Forest trained on log-transformed sales (better for skewed data)\")\n",
    "    print(f\"    - Reduced overfitting: max_depth=10, min_samples_split=20, min_samples_leaf=10\")\n",
    "    print(f\"    - Added max_features='sqrt' and max_samples=0.8 for regularization\")\n",
    "    print(f\"    - ARIMA coverage increased from 100 to 200 top combinations\")\n",
    "    print(f\"    - ARIMA weight increased from 30% to 50% where available\")\n",
    "    print(f\"    - All lag features computed in log space for consistency\")\n",
    "    print(f\"    - Enhanced ARIMA with more parameter configurations\")\n",
    "    \n",
    "    print(\">>> Submission saved as 'enhanced_rf_arima_log_submission.csv'\")\n",
    "    print(\">>> Expected improvements:\")\n",
    "    print(\">>>   1. Better handling of sales distribution via log transform\")\n",
    "    print(\">>>   2. Reduced overfitting in Random Forest\")\n",
    "    print(\">>>   3. Stronger time series patterns from enhanced ARIMA\")\n",
    "    print(\">>>   4. More robust ensemble with increased ARIMA influence\")\n",
    "    \n",
    "else:\n",
    "    print(\">>> ERROR: No predictions generated!\")\n",
    "\n",
    "# ============================\n",
    "# üîü Additional diagnostic outputs\n",
    "# ============================\n",
    "if rf_predictions is not None and len(arima_df) > 0:\n",
    "    print(f\"\\n>>> MODEL DIAGNOSTICS:\")\n",
    "    print(f\">>> Training log_sales stats: mean={train_fe['log_sales'].mean():.3f}, std={train_fe['log_sales'].std():.3f}\")\n",
    "    \n",
    "    # Check prediction quality\n",
    "    rf_pred_stats = f\"RF predictions: mean={np.mean(rf_predictions):.2f}, median={np.median(rf_predictions):.2f}\"\n",
    "    arima_pred_stats = f\"ARIMA predictions: mean={arima_df['sales_arima'].mean():.2f}, median={arima_df['sales_arima'].median():.2f}\"\n",
    "    \n",
    "    print(f\">>> {rf_pred_stats}\")\n",
    "    print(f\">>> {arima_pred_stats}\")\n",
    "    \n",
    "    # Check for extreme predictions\n",
    "    extreme_high = (final_submission['sales'] > final_submission['sales'].quantile(0.99)).sum()\n",
    "    extreme_low = (final_submission['sales'] < 0.1).sum()\n",
    "    print(f\">>> Extreme predictions: {extreme_high} high outliers, {extreme_low} very low\")\n",
    "    \n",
    "print(\">>> Enhanced pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d245dcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "sales                       float64\n",
       "onpromotion                   int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "isHoliday                     int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "date                 datetime64[ns]\n",
       "store_nbr                     int64\n",
       "family                       object\n",
       "onpromotion                   int64\n",
       "earthquake_impact             int64\n",
       "salary_day_impact             int64\n",
       "isHoliday                     int64\n",
       "oil_price                   float64\n",
       "city                         object\n",
       "state                        object\n",
       "type                         object\n",
       "cluster                       int64\n",
       "transactions                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f133a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n",
      "Infinite values per column:\n",
      " id       0\n",
      "sales    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Check for NaN\n",
    "print(\"NaN values per column:\\n\", submission.isna().sum())\n",
    "\n",
    "# Check for inf\n",
    "print(\"Infinite values per column:\\n\", np.isinf(submission).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb04ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, sales]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"my_submission_clean.csv\")\n",
    "\n",
    "# Find rows where sales is infinite\n",
    "mask = np.isinf(submission[\"sales\"])\n",
    "\n",
    "# Show the offending rows\n",
    "print(submission.loc[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b51f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
      "/var/folders/fv/7g0st0sd2gj1ztjzwt7t0s2c0000gn/T/ipykernel_60960/1686795528.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Replace inf with NaN first (so ffill works)\n",
    "submission[\"sales\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Forward fill\n",
    "submission[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# (Optional) if the very first row is NaN/inf, ffill won‚Äôt work ‚Äî so backfill as fallback\n",
    "submission[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Save cleaned file\n",
    "submission.to_csv(\"my_submission_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42f2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Test set ID range: 3000888 to 3029399\n",
      "‚ñ∂Ô∏è Submission ID range: 3000888 to 3029399\n",
      "‚úÖ All submission IDs are inside test_df.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your files\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "submission = pd.read_csv(\"my_submission.csv\")\n",
    "\n",
    "# Check the range of IDs\n",
    "print(\"‚ñ∂Ô∏è Test set ID range:\", test_df[\"id\"].min(), \"to\", test_df[\"id\"].max())\n",
    "print(\"‚ñ∂Ô∏è Submission ID range:\", submission[\"id\"].min(), \"to\", submission[\"id\"].max())\n",
    "\n",
    "# Also check for IDs in submission but not in test\n",
    "extra_ids = set(submission[\"id\"]) - set(test_df[\"id\"])\n",
    "if extra_ids:\n",
    "    print(\"‚ö†Ô∏è IDs present in submission but not in test:\", list(extra_ids)[:10], \"...\")\n",
    "else:\n",
    "    print(\"‚úÖ All submission IDs are inside test_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
